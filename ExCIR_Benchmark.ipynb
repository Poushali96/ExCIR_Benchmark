{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTj8R2XbcA-W"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # ExCIR — Cross-Domain Benchmarks (Anonymous, IEEE Reproducibility)\n",
        "#\n",
        "# This notebook reproduces the ExCIR experiments across **29 datasets**, comparing against\n",
        "# common global attribution baselines. It is self-contained and robust:\n",
        "#\n",
        "# - Deterministic seeding, single-run reproducibility\n",
        "# - XGBoost GPU acceleration when available, graceful CPU fallback\n",
        "# - OpenML/text loaders with try/except → synthetic fallback\n",
        "# - Clean CSV outputs in `out/` + paper-style figures\n",
        "#\n",
        "# **Methods compared**: ExCIR (ours), TreeGain, PFI, PDP-var, MI(pred), MI(label), Surrogate-LR\n",
        "# **Figures**: AOPC curves, stability histograms, Pareto cost-agreement, agreement bundle\n",
        "# **Tables**: base model metrics, feature_importances, AOPC summary/curves, top-k sufficiency, stability, lightweight checks\n",
        "\n",
        "# %%\n",
        "# ============ 0) Environment & Determinism ============\n",
        "import os, sys, csv, time, warnings, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.stats import gaussian_kde, entropy\n",
        "from scipy.signal import periodogram\n",
        "\n",
        "# GPU-capable model (falls back to CPU automatically in our code)\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "\n",
        "# Safe imports for datasets (avoid recursive naming)\n",
        "from sklearn.datasets import (\n",
        "    fetch_openml,\n",
        "    fetch_20newsgroups,\n",
        "    fetch_california_housing,\n",
        "    load_diabetes as skl_load_diabetes,\n",
        "    load_digits as skl_load_digits,\n",
        "    load_iris as skl_load_iris,\n",
        "    load_wine as skl_load_wine,\n",
        "    load_breast_cancer as skl_load_breast_cancer,\n",
        "    make_classification,\n",
        "    make_regression,\n",
        ")\n",
        "\n",
        "sys.setrecursionlimit(3000)\n",
        "\n",
        "def set_seed(seed: int = 0, threads: int = 1):\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    os.environ[\"OMP_NUM_THREADS\"] = str(threads)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "set_seed(0, threads=1)\n",
        "print(\"Determinism set.\")\n",
        "\n",
        "# %%\n",
        "# ============ 1) ExCIR core (CIR) & BlockCIR ============\n",
        "\n",
        "def _center_midmean(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Robust centering via mid-mean (average of Q1, Q3).\"\"\"\n",
        "    q1x = np.quantile(X, 0.25, axis=0)\n",
        "    q3x = np.quantile(X, 0.75, axis=0)\n",
        "    Xc = X - 0.5 * (q1x + q3x)\n",
        "    q1y, q3y = np.quantile(y, [0.25, 0.75])\n",
        "    yc = y - 0.5 * (q1y + q3y)\n",
        "    return Xc, yc\n",
        "\n",
        "def _center_median(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    return X - np.median(X, axis=0), y - np.median(y)\n",
        "\n",
        "def _center_mean(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    return X - X.mean(axis=0), y - y.mean()\n",
        "\n",
        "def center_cols(X, y, how=\"midmean\"):\n",
        "    if how == \"midmean\": return _center_midmean(X, y)\n",
        "    if how == \"median\":  return _center_median(X, y)\n",
        "    if how == \"mean\":    return _center_mean(X, y)\n",
        "    raise ValueError(f\"Unknown centering: {how}\")\n",
        "\n",
        "def excir_feature_scores(X: np.ndarray, preds: np.ndarray, centering=\"midmean\") -> np.ndarray:\n",
        "    \"\"\"ExCIR per-feature score: 0.5*(1 + sum(x_tilde*y_tilde) / sum(|x_tilde*y_tilde|)).\"\"\"\n",
        "    Xc, yc = center_cols(np.asarray(X, float), np.asarray(preds, float).ravel(), centering)\n",
        "    P = Xc * yc[:, None]\n",
        "    N = P.sum(axis=0)\n",
        "    D = np.abs(P).sum(axis=0)\n",
        "    out = 0.5 * (1.0 + np.divide(N, D, out=np.zeros_like(N), where=D>0))\n",
        "    out[D == 0] = 0.5\n",
        "    return out\n",
        "\n",
        "def blockcir_scores(X: np.ndarray, preds: np.ndarray, blocks: Dict[str, List[int]], centering=\"midmean\") -> Dict[str, float]:\n",
        "    \"\"\"Group-wise ExCIR over correlated feature blocks.\"\"\"\n",
        "    Xc, yc = center_cols(np.asarray(X, float), np.asarray(preds, float).ravel(), centering)\n",
        "    P = Xc * yc[:, None]\n",
        "    scores = {}\n",
        "    for name, idxs in blocks.items():\n",
        "        idxs = np.asarray(list(idxs), dtype=int)\n",
        "        pG = P[:, idxs].sum(axis=1)\n",
        "        uG = np.abs(P[:, idxs]).sum(axis=1)\n",
        "        NG, DG = pG.sum(), uG.sum()\n",
        "        scores[name] = 0.5 * (1.0 + (NG / DG)) if DG > 0 else 0.5\n",
        "    return scores\n",
        "\n",
        "def expand_block_to_features(block_scores: Dict[str, float], d: int, blocks: Dict[str, List[int]]) -> np.ndarray:\n",
        "    vals = np.zeros(d)\n",
        "    for name, idxs in blocks.items():\n",
        "        for j in idxs:\n",
        "            vals[j] = block_scores[name]\n",
        "    return vals\n",
        "\n",
        "# %%\n",
        "# ============ 2) Agreement Metrics & AOPC ============\n",
        "\n",
        "def jaccard_at_k(a_idx, b_idx, k: int):\n",
        "    A, B = set(a_idx[:k]), set(b_idx[:k])\n",
        "    u = len(A | B)\n",
        "    return (len(A & B) / u) if u else 1.0\n",
        "\n",
        "def precision_at_k(ref_idx, cand_idx, k: int):\n",
        "    A, B = set(ref_idx[:k]), set(cand_idx[:k])\n",
        "    return len(A & B) / k\n",
        "\n",
        "def spearman_rank_corr(a: np.ndarray, b: np.ndarray):\n",
        "    \"\"\"Fast Spearman using rank-of-rank trick.\"\"\"\n",
        "    ra = a.argsort().argsort().astype(float)\n",
        "    rb = b.argsort().argsort().astype(float)\n",
        "    ra = (ra - ra.mean()) / (ra.std() if ra.std() else 1.0)\n",
        "    rb = (rb - rb.mean()) / (rb.std() if rb.std() else 1.0)\n",
        "    return float((ra * rb).mean())\n",
        "\n",
        "def projection_alignment_residual(y_full, y_light):\n",
        "    \"\"\"Affine residual between y_full and y_light after best affine fit.\"\"\"\n",
        "    y = np.asarray(y_full).ravel(); yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]\n",
        "    Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full, y_light, grid_points=400):\n",
        "    \"\"\"Symmetric KL between KDEs of y_full and y_light.\"\"\"\n",
        "    y = np.asarray(y_full).ravel(); yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y); kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]; lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None); q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo)/max(grid_points-1, 1)\n",
        "    kl_pq = float(np.sum(p*(np.log(p)-np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q*(np.log(q)-np.log(p))) * dx)\n",
        "    return 0.5*(kl_pq + kl_qp)\n",
        "\n",
        "def rank_from_scores(scores):\n",
        "    return np.argsort(-scores)\n",
        "\n",
        "def aopc_curves(task, model, Xte, yte, scores, steps=9):\n",
        "    \"\"\"Insertion↑ / Deletion↓ curves by masking/unmasking top-k features.\"\"\"\n",
        "    idx = rank_from_scores(scores)\n",
        "    n, d = Xte.shape\n",
        "    fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f*d))\n",
        "        keep = idx[:k]\n",
        "        mask = np.zeros(d, bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0).astype(np.float32)  # reveal top-k\n",
        "        X_del = np.where(mask, 0.0, Xte).astype(np.float32)  # remove top-k\n",
        "        y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "        if task == \"clf\":\n",
        "            ins.append(accuracy_score(yte, y_ins)); dele.append(accuracy_score(yte, y_del))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, y_ins)); dele.append(r2_score(yte, y_del))\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "def stability_noise_eval(scores_fn, Xva, p_va, repeats=15, sigma=0.01):\n",
        "    base = scores_fn(Xva, p_va)\n",
        "    cirs, j8 = [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=Xva.shape)\n",
        "        s_noisy = scores_fn(Xva + noise, p_va)\n",
        "        cirs.append(spearman_rank_corr(base, s_noisy))\n",
        "        j8.append(jaccard_at_k(base.argsort()[::-1], s_noisy.argsort()[::-1], k=min(8, Xva.shape[1])))\n",
        "    return np.asarray(cirs), np.asarray(j8)\n",
        "\n",
        "# %%\n",
        "# ============ 3) Baselines (PFI, TreeGain, PDP-var, MI, Surrogate-LR) ============\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats=5, seed=0):\n",
        "    try:\n",
        "        return permutation_importance(model, X.astype(np.float32), y,\n",
        "                                      n_repeats=n_repeats, random_state=seed,\n",
        "                                      scoring=None).importances_mean\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[1])\n",
        "\n",
        "def tree_gain_scores(model, d):\n",
        "    try:\n",
        "        w = getattr(model, \"feature_importances_\", None)\n",
        "        return w if (w is not None and len(w)==d) else np.zeros(d)\n",
        "    except Exception:\n",
        "        return np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points=15, random_state=0):\n",
        "    d = X.shape[1]; scores = np.zeros(d)\n",
        "    rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(1000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(model, Xs.astype(np.float32), features=[j],\n",
        "                                    kind='average', grid_resolution=grid_points)\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha=1.0):\n",
        "    X = np.asarray(X, float); y = np.asarray(teacher_scores, float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0).fit(X, y)\n",
        "    w = np.abs(reg.coef_); w = np.linalg.norm(w, axis=0) if w.ndim>1 else w\n",
        "    s = w/(w.sum()+1e-12)\n",
        "    return s\n",
        "\n",
        "def mi_pred_scores(X, preds, task):\n",
        "    X = np.asarray(X, float); p = np.asarray(preds, float).ravel()\n",
        "    if task==\"clf\":\n",
        "        z = (p > np.median(p)).astype(int);\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "    return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task):\n",
        "    return mutual_info_classif(X, y, random_state=0) if task==\"clf\" else mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# %%\n",
        "# ============ 4) XGBoost Trainer with GPU → CPU Fallback ============\n",
        "\n",
        "def _get_xgb_model(task: str, y_data: np.ndarray, params: dict):\n",
        "    unique_classes = np.unique(y_data)\n",
        "    if task == \"clf\":\n",
        "        if len(unique_classes) > 2:\n",
        "            return xgb.XGBClassifier(objective='multi:softmax', num_class=len(unique_classes),\n",
        "                                     eval_metric='merror', **params)\n",
        "        else:\n",
        "            return xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
        "                                     use_label_encoder=False, **params)\n",
        "    else:\n",
        "        return xgb.XGBRegressor(objective='reg:squarederror', eval_metric='rmse', **params)\n",
        "\n",
        "def train_gboost(task, Xtr, ytr, Xva, yva, seed=0):\n",
        "    Xall = np.vstack([Xtr, Xva]).astype(np.float32)\n",
        "    yall = np.r_[ytr, yva]\n",
        "    params = dict(n_estimators=50, random_state=seed, tree_method='gpu_hist',\n",
        "                  gpu_id=0, validate_parameters=True, n_jobs=1)\n",
        "    m = _get_xgb_model(task, yall, params)\n",
        "    try:\n",
        "        m.fit(Xall, yall); return m\n",
        "    except xgb.core.XGBoostError:\n",
        "        print(\"  [XGBoost] GPU failed. Falling back to CPU 'hist'.\")\n",
        "        params['tree_method'] = 'hist'; params.pop('gpu_id', None)\n",
        "        m = _get_xgb_model(task, yall, params)\n",
        "        try:\n",
        "            m.fit(Xall, yall); return m\n",
        "        except Exception as e_cpu:\n",
        "            print(f\"  [XGBoost] CPU fallback failed: {e_cpu}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"  [XGBoost] Training failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def predict_scores(task, model, X):\n",
        "    if model is None: return np.zeros(X.shape[0])\n",
        "    X = X.astype(np.float32)\n",
        "    try:\n",
        "        if task==\"clf\":\n",
        "            # robustly get probabilities when available; else predictions\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                proba = model.predict_proba(X)\n",
        "                if proba.ndim == 2:\n",
        "                    if proba.shape[1] == 2:\n",
        "                        return proba[:, 1]\n",
        "                    return proba[:, -1]\n",
        "            pred = model.predict(X)\n",
        "            # if predict() returns class labels, coerce to [0,1] for ExCIR (best-effort)\n",
        "            if pred.ndim == 1 and np.issubdtype(pred.dtype, np.integer):\n",
        "                return (pred == pred.max()).astype(float)\n",
        "            return pred\n",
        "        return model.predict(X)\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[0])\n",
        "\n",
        "# %%\n",
        "# ============ 5) Dataset Plumbing & 29 Loaders (with fallbacks) ============\n",
        "\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str  # \"clf\" | \"reg\"\n",
        "    Xtr: np.ndarray; ytr: np.ndarray\n",
        "    Xva: np.ndarray; yva: np.ndarray\n",
        "    Xte: np.ndarray; yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def _split_xy(X, y, task, seed=0):\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2,\n",
        "                                          stratify=y if task==\"clf\" else None,\n",
        "                                          random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.25,\n",
        "                                          stratify=ytr if task==\"clf\" else None,\n",
        "                                          random_state=seed)\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names\n",
        "\n",
        "def _split_pack_from_xy(X, y, name, task, seed):\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, task, seed)\n",
        "    return DatasetPack(name, task, Xtr, ytr, Xva, yva, Xte, yte, feat)\n",
        "\n",
        "# --- Textish / OpenMLish with fallbacks ---\n",
        "def load_20ng_binary(seed=0):\n",
        "    try:\n",
        "        cats=['comp.graphics','sci.space']\n",
        "        tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "        te = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n",
        "        tf = TfidfVectorizer(max_features=3000, ngram_range=(1,2), stop_words='english')\n",
        "        Xtr_full = tf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "        Xte = tf.transform(te.data).astype(np.float32).toarray()\n",
        "        ytr_full, yte = tr.target, te.target\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2,\n",
        "                                              stratify=ytr_full, random_state=seed)\n",
        "        feat = [f\"tfidf_{i}\" for i in range(Xtr.shape[1])]\n",
        "        return DatasetPack(\"20ng_bin\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,feat)\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_adult(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "        dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "        y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "        X = dfX.to_numpy(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"adult\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,list(dfX.columns))\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_har6(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "        X = ds.data.to_numpy(float); y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "        return _split_pack_from_xy(X, y, \"har6\", \"clf\", seed)\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_digits8x8(seed=0):\n",
        "    dg = skl_load_digits()\n",
        "    X = dg.data.astype(float)/16.0; y = dg.target\n",
        "    return _split_pack_from_xy(X, y, \"digits8x8\", \"clf\", seed)\n",
        "\n",
        "def load_california(seed=0):\n",
        "    try:\n",
        "        ds = fetch_california_housing()\n",
        "        X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "        return _split_pack_from_xy(X, y, \"california\", \"reg\", seed)\n",
        "    except Exception:\n",
        "        return load_synthetic_reg(seed)\n",
        "\n",
        "def load_diabetes(seed=0):\n",
        "    ds = skl_load_diabetes()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    feat = list(getattr(ds, \"feature_names\", [f\"f{i}\" for i in range(X.shape[1])]))\n",
        "    pack = _split_pack_from_xy(X, y, \"diabetes\", \"reg\", seed)\n",
        "    return DatasetPack(pack.name, pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, pack.Xte, pack.yte, feat)\n",
        "\n",
        "def load_iris(seed=0):\n",
        "    ds = skl_load_iris()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    feat = list(ds.feature_names)\n",
        "    pack = _split_pack_from_xy(X, y, \"iris\", \"clf\", seed)\n",
        "    return DatasetPack(pack.name, pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, pack.Xte, pack.yte, feat)\n",
        "\n",
        "def load_wine(seed=0):\n",
        "    ds = skl_load_wine()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    feat = list(ds.feature_names)\n",
        "    pack = _split_pack_from_xy(X, y, \"wine\", \"clf\", seed)\n",
        "    return DatasetPack(pack.name, pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, pack.Xte, pack.yte, feat)\n",
        "\n",
        "def load_breast_cancer(seed=0):\n",
        "    ds = skl_load_breast_cancer()\n",
        "    X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    feat = list(ds.feature_names)\n",
        "    pack = _split_pack_from_xy(X, y, \"breast_cancer\", \"clf\", seed)\n",
        "    return DatasetPack(pack.name, pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, pack.Xte, pack.yte, feat)\n",
        "\n",
        "def load_credit_g(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"credit-g\", version=1, as_frame=True)\n",
        "        dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "        y = (ds.target == 'good').astype(int).to_numpy()\n",
        "        X = dfX.to_numpy(float)\n",
        "        return _split_pack_from_xy(X, y, \"credit_g\", \"clf\", seed)\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_covertype_small(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"Covertype\", version=3, as_frame=True)\n",
        "        X = ds.data.select_dtypes(include=[np.number]).to_numpy(float)\n",
        "        y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "        # downsample to keep runtime reasonable\n",
        "        rs = check_random_state(seed)\n",
        "        idx = rs.choice(np.arange(X.shape[0]), size=min(15000, X.shape[0]), replace=False)\n",
        "        return _split_pack_from_xy(X[idx], y[idx], \"covertype_small\", \"clf\", seed)\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_phishing(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"PhishingWebsites\", version=1, as_frame=True)\n",
        "        dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "        X = dfX.to_numpy(float)\n",
        "        y = (ds.target == 'phishing').astype(int).to_numpy()\n",
        "        return _split_pack_from_xy(X, y, \"phishing\", \"clf\", seed)\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "# --- Synthetic families + feature-like modalities ---\n",
        "def load_synthetic_clf(seed=0):\n",
        "    X, y = make_classification(n_samples=3000, n_features=40, n_informative=10,\n",
        "                               n_redundant=10, n_repeated=5, n_classes=2, random_state=seed)\n",
        "    return _split_pack_from_xy(X, y, \"synthetic_clf\", \"clf\", seed)\n",
        "\n",
        "def load_synthetic_reg(seed=0):\n",
        "    X, y = make_regression(n_samples=3000, n_features=40, n_informative=10, noise=1.5, random_state=seed)\n",
        "    return _split_pack_from_xy(X, y, \"synthetic_reg\", \"reg\", seed)\n",
        "\n",
        "def _dominant_freq(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs)\n",
        "    return f[np.argmax(Pxx)] if len(Pxx)>0 and np.isfinite(Pxx).any() else 0.0\n",
        "\n",
        "def _spec_entropy(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs); p = Pxx + 1e-12; p /= p.sum()\n",
        "    return float(entropy(p))\n",
        "\n",
        "def _ts_to_feats(T, fs=1.0):\n",
        "    T = np.asarray(T, float)\n",
        "    return np.array([T.mean(), T.std(), T.min(), T.max(),\n",
        "                     _dominant_freq(T, fs), _spec_entropy(T, fs),\n",
        "                     np.mean(np.diff(T)), np.std(np.diff(T))], float)\n",
        "\n",
        "def _gen_ts_dataset(n_samples=2000, length=256, n_channels=1, seed=0, task=\"clf\"):\n",
        "    rs = check_random_state(seed); fs=1.0; X_feats=[]; y=[]\n",
        "    for _ in range(n_samples):\n",
        "        label = rs.randint(0,2) if task==\"clf\" else None\n",
        "        chans=[]\n",
        "        for _c in range(n_channels):\n",
        "            freq = 0.05 + 0.2*rs.rand() + (0.1 if (task==\"clf\" and label==1) else 0.0)\n",
        "            t = np.arange(length)/fs\n",
        "            sig = (np.sin(2*np.pi*freq*t)\n",
        "                   + 0.5*np.sin(2*np.pi*(freq*2)*t + rs.rand()*2*np.pi)\n",
        "                   + 0.3*rs.randn(length))\n",
        "            chans.append(sig)\n",
        "        chans = np.stack(chans, axis=0)\n",
        "        feats = np.concatenate([_ts_to_feats(ch, fs) for ch in chans], 0); X_feats.append(feats)\n",
        "        y.append(label if task==\"clf\" else feats.sum()+rs.randn()*0.5)\n",
        "    X = np.vstack(X_feats); y = np.array(y, dtype=int if task==\"clf\" else float)\n",
        "    return X, y\n",
        "\n",
        "def load_ecg_heartbeat(seed=0):\n",
        "    X, y = _gen_ts_dataset(2500, 256, 1, seed, \"clf\")\n",
        "    return _split_pack_from_xy(X, y, \"ecg_heartbeat\", \"clf\", seed)\n",
        "\n",
        "def load_fashion_mnist_features(seed=0):\n",
        "    rs = check_random_state(seed); n=5000; d=128; y=rs.randint(0,10,size=n)\n",
        "    Z = rs.randn(n,16) + (y[:,None]*0.15); W = rs.randn(16,d)\n",
        "    X = Z@W + 0.1*rs.randn(n,d)\n",
        "    return _split_pack_from_xy(X, y, \"fashion_mnist_features\", \"clf\", seed)\n",
        "\n",
        "def load_protein_structure(seed=0):\n",
        "    X, y = make_regression(4000,120,20,noise=2.0,random_state=seed)\n",
        "    return _split_pack_from_xy(X, y, \"protein_structure\", \"reg\", seed)\n",
        "\n",
        "def load_stock_technical(seed=0):\n",
        "    rs=check_random_state(seed); n=4000; T=60\n",
        "    prices = 100 + np.cumsum(rs.randn(n,T),1); rets = np.diff(prices,1)\n",
        "    vol=rets.std(1); mom=prices[:,-1]-prices[:,0]\n",
        "    skew=((rets-rets.mean(1,keepdims=True))**3).mean(1); kurt=((rets-rets.mean(1,keepdims=True))**4).mean(1)\n",
        "    feats=np.c_[rets.mean(1),vol,mom,skew,kurt]; y=(rs.randn(n)+0.2*np.sign(mom)>0).astype(int)\n",
        "    return _split_pack_from_xy(feats,y,\"stock_technical\",\"clf\",seed)\n",
        "\n",
        "def load_sensor_fusion(seed=0):\n",
        "    X, y = _gen_ts_dataset(3000,128,3,seed,\"clf\")\n",
        "    return _split_pack_from_xy(X, y, \"sensor_fusion\", \"clf\", seed)\n",
        "\n",
        "def load_gene_expression(seed=0):\n",
        "    rs=check_random_state(seed); n=800; d=2000; y=rs.randint(0,2,size=n)\n",
        "    base=rs.randn(n,d)*0.5; signal=np.zeros((n,d))\n",
        "    idx=rs.choice(d,30,replace=False); signal[y==1][:,idx]=1.5\n",
        "    X=base+signal+0.1*rs.randn(n,d)\n",
        "    return _split_pack_from_xy(X,y,\"gene_expression\",\"clf\",seed)\n",
        "\n",
        "def load_network_topology(seed=0):\n",
        "    rs=check_random_state(seed); n=3000\n",
        "    deg=rs.gamma(2.0,2.0,n); cl=rs.beta(2,5,n); asst=rs.uniform(-0.5,0.5,n); tri=rs.poisson(5,n); btw=rs.exponential(1.0,n)\n",
        "    X=np.c_[deg,cl,asst,tri,btw]; y=(0.3*deg+0.8*cl-0.5*asst+0.1*tri+0.2*btw+rs.randn(n)*0.5>1.8).astype(int)\n",
        "    return _split_pack_from_xy(X,y,\"network_topology\",\"clf\",seed)\n",
        "\n",
        "def load_audio_mfcc(seed=0):\n",
        "    rs=check_random_state(seed); n=3500; bands=20; y=rs.randint(0,5,size=n); base=rs.randn(n,bands)\n",
        "    for c in range(5): base[y==c]+=(c-2)*0.25\n",
        "    X=base+0.1*rs.randn(n,bands)\n",
        "    return _split_pack_from_xy(X,y,\"audio_mfcc\",\"clf\",seed)\n",
        "\n",
        "def load_weather_station(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    pressure=rs.normal(1013,8,n); humidity=rs.uniform(15,95,n); wind=rs.gamma(2.0,1.2,n); cloud=rs.uniform(0,1,n); doy=rs.randint(1,366,n)\n",
        "    temp = 10 + 10*np.sin(2*np.pi*doy/365) - 0.01*(pressure-1013) - 0.05*(humidity-50) - 0.3*cloud + 0.2*wind + rs.randn(n)\n",
        "    X=np.c_[pressure,humidity,wind,cloud,doy]; y=temp\n",
        "    return _split_pack_from_xy(X,y,\"weather_station\",\"reg\",seed)\n",
        "\n",
        "def load_eeg_signals(seed=0):\n",
        "    X, y = _gen_ts_dataset(3000,256,4,seed,\"clf\")\n",
        "    return _split_pack_from_xy(X,y,\"eeg_signals\",\"clf\",seed)\n",
        "\n",
        "def load_satellite_ndvi(seed=0):\n",
        "    rs=check_random_state(seed); n=4000\n",
        "    bands=rs.uniform(0,1,(n,6)); sun=rs.uniform(0,70,n); view=rs.uniform(0,30,n)\n",
        "    ndvi=(bands[:,3]-bands[:,2])/(bands[:,3]+bands[:,2]+1e-6); y=ndvi+0.001*(sun-35)-0.001*(view-15)+rs.randn(n)*0.02\n",
        "    X=np.c_[bands,sun,view]\n",
        "    return _split_pack_from_xy(X,y,\"satellite_ndvi\",\"reg\",seed)\n",
        "\n",
        "def load_industrial_process(seed=0):\n",
        "    rs=check_random_state(seed); n=4500\n",
        "    s1=rs.normal(0,1,n); s2=rs.normal(0,1.2,n); s3=rs.normal(0.5,0.8,n); sp=rs.uniform(-1,1,n); noise=rs.randn(n)*0.3\n",
        "    y=2.0+1.5*s1-0.7*s2+0.9*s3+1.2*sp+noise; X=np.c_[s1,s2,s3,sp]\n",
        "    return _split_pack_from_xy(X,y,\"industrial_process\",\"reg\",seed)\n",
        "\n",
        "def load_social_network(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    posts=rs.poisson(3,n); friends=rs.poisson(120,n); likes=rs.poisson(50,n); comments=rs.poisson(12,n); shares=rs.poisson(5,n)\n",
        "    X=np.c_[posts,friends,likes,comments,shares]\n",
        "    y=(0.01*friends+0.03*likes+0.05*comments+0.08*shares+0.2*posts+rs.randn(n)*0.5>3.0).astype(int)\n",
        "    return _split_pack_from_xy(X,y,\"social_network\",\"clf\",seed)\n",
        "\n",
        "def load_cyber_security(seed=0):\n",
        "    X,y=make_classification(5000,30,8,10,weights=[0.8,0.2],class_sep=1.5,random_state=seed)\n",
        "    return _split_pack_from_xy(X,y,\"cyber_security\",\"clf\",seed)\n",
        "\n",
        "def load_mixed_modal(seed=0):\n",
        "    rs=check_random_state(seed)\n",
        "    X_tab,y=make_classification(4000,20,6,6,random_state=seed); X_spec=[]\n",
        "    for _ in range(X_tab.shape[0]):\n",
        "        t=np.arange(64); sig=np.sin(2*np.pi*(0.08+0.1*rs.rand())*t)+0.3*rs.randn(64)\n",
        "        X_spec.append(_ts_to_feats(sig))\n",
        "    X=np.c_[X_tab,np.vstack(X_spec)]\n",
        "    return _split_pack_from_xy(X,y,\"mixed_modal\",\"clf\",seed)\n",
        "\n",
        "# --- Registry: 29 datasets ---\n",
        "ALL_DATASETS = {\n",
        "    # OpenML/text + classics\n",
        "    \"20ng_bin\":load_20ng_binary, \"adult\":load_adult, \"har6\":load_har6,\n",
        "    \"digits8x8\":load_digits8x8, \"california\":load_california, \"diabetes\":load_diabetes,\n",
        "    \"iris\":load_iris, \"wine\":load_wine, \"breast_cancer\":load_breast_cancer,\n",
        "    \"credit_g\":load_credit_g, \"covertype_small\":load_covertype_small, \"phishing\":load_phishing,\n",
        "    # synthetic & modality-like\n",
        "    \"synthetic_clf\":load_synthetic_clf, \"synthetic_reg\":load_synthetic_reg,\n",
        "    \"ecg_heartbeat\":load_ecg_heartbeat, \"fashion_mnist_features\":load_fashion_mnist_features,\n",
        "    \"protein_structure\":load_protein_structure, \"stock_technical\":load_stock_technical,\n",
        "    \"sensor_fusion\":load_sensor_fusion, \"gene_expression\":load_gene_expression,\n",
        "    \"network_topology\":load_network_topology, \"audio_mfcc\":load_audio_mfcc,\n",
        "    \"weather_station\":load_weather_station, \"eeg_signals\":load_eeg_signals,\n",
        "    \"satellite_ndvi\":load_satellite_ndvi, \"industrial_process\":load_industrial_process,\n",
        "    \"social_network\":load_social_network, \"cyber_security\":load_cyber_security,\n",
        "    \"mixed_modal\":load_mixed_modal,\n",
        "}\n",
        "assert len(ALL_DATASETS) == 29, f\"Expect 29 datasets, got {len(ALL_DATASETS)}\"\n",
        "\n",
        "# %%\n",
        "# ============ 6) CSV & Plot Helpers ============\n",
        "\n",
        "def ensure_out(outdir=\"out\"):\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def append_rows(path: str, header: List[str], rows: List[List]):\n",
        "    write_header = not os.path.exists(path)\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if write_header: wr.writerow(header)\n",
        "        if rows: wr.writerows(rows)\n",
        "\n",
        "# %%\n",
        "# ============ 7) Per-dataset Pipeline ============\n",
        "\n",
        "def run_on_dataset(pack: DatasetPack, ks=(3,5,8,12), seed: int = 0,\n",
        "                   do_pfi: bool = True, outdir=\"out\") -> Dict[str, np.ndarray]:\n",
        "    ensure_out(outdir)\n",
        "    Ntot = pack.Xtr.shape[0]+pack.Xva.shape[0]+pack.Xte.shape[0]\n",
        "    print(f\"[{pack.name}] task={pack.task}, N={Ntot}, D={pack.Xtr.shape[1]}\")\n",
        "\n",
        "    # (1) Train base model\n",
        "    base = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base is None:\n",
        "        print(f\"  -> skip: base model failed.\")\n",
        "        return {}\n",
        "\n",
        "    # (2) Base metric\n",
        "    yhat_test = base.predict(pack.Xte.astype(np.float32))\n",
        "    base_metric = accuracy_score(pack.yte, yhat_test) if pack.task==\"clf\" else r2_score(pack.yte, yhat_test)\n",
        "    append_rows(os.path.join(outdir, \"base_model_metrics.csv\"),\n",
        "                [\"dataset\",\"task\",\"test_metric_name\",\"test_metric_value\"],\n",
        "                [[pack.name, pack.task, \"accuracy\" if pack.task==\"clf\" else \"r2\", base_metric]])\n",
        "\n",
        "    # (3) Scores for all methods\n",
        "    p_va = predict_scores(pack.task, base, pack.Xva)\n",
        "    scores: Dict[str, np.ndarray] = {\n",
        "        \"ExCIR\":        excir_feature_scores(pack.Xva, p_va),\n",
        "        \"TreeGain\":     tree_gain_scores(base, pack.Xva.shape[1]),\n",
        "        \"PDP-var\":      pdp_var_scores(base, pack.Xva),\n",
        "        \"MI(pred)\":     mi_pred_scores(pack.Xva, p_va, task=pack.task),\n",
        "        \"MI(label)\":    mi_label_scores(pack.Xva, pack.yva, task=pack.task),\n",
        "        \"Surrogate-LR\": surrogate_lr_scores(pack.Xva, p_va),\n",
        "    }\n",
        "    if do_pfi:\n",
        "        scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=5, seed=seed)\n",
        "\n",
        "    # (4) Export feature importances\n",
        "    feat_rows = []\n",
        "    for mname, vec in scores.items():\n",
        "        for j, val in enumerate(vec):\n",
        "            fname = pack.feature_names[j] if j < len(pack.feature_names) else f\"f{j}\"\n",
        "            feat_rows.append([pack.name, mname, j, fname, float(val)])\n",
        "    append_rows(os.path.join(outdir, \"feature_importances.csv\"),\n",
        "                [\"dataset\",\"method\",\"feature_index\",\"feature_name\",\"score\"],\n",
        "                feat_rows)\n",
        "\n",
        "    # (5) AOPC (insertion/deletion)\n",
        "    curves_rows, summary_rows = [], []\n",
        "    for mname, scr in scores.items():\n",
        "        fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scr, steps=9)\n",
        "        for f, a, d in zip(fr, ins, dele):\n",
        "            curves_rows.append([pack.name, mname, float(f), float(a), float(d)])\n",
        "        auc_ins = float(np.trapz(ins, fr))\n",
        "        auc_del = float(np.trapz(dele, fr))\n",
        "        combined = 0.5*(auc_ins + (1.0 - auc_del))\n",
        "        summary_rows.append([pack.name, mname, auc_ins, auc_del, combined])\n",
        "    append_rows(os.path.join(outdir, \"aopc_curves.csv\"),\n",
        "                [\"dataset\",\"method\",\"fraction_kept\",\"insertion\",\"deletion\"], curves_rows)\n",
        "    append_rows(os.path.join(outdir, \"aopc_summary.csv\"),\n",
        "                [\"dataset\",\"method\",\"auc_insertion\",\"auc_deletion\",\"combined\"], summary_rows)\n",
        "\n",
        "    # (6) Top-k sufficiency (retrain on top-k features by each method)\n",
        "    topk_rows = []\n",
        "    for m, v in scores.items():\n",
        "        ranks = rank_from_scores(v)\n",
        "        for k in ks:\n",
        "            keep = ranks[:k]\n",
        "            params = dict(n_estimators=100, random_state=seed, tree_method='gpu_hist',\n",
        "                          gpu_id=0, validate_parameters=True, n_jobs=1)\n",
        "            Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep].astype(np.float32)\n",
        "            ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "            model_k = _get_xgb_model(pack.task, ytr_k, params)\n",
        "            try:\n",
        "                model_k.fit(Xtr_k, ytr_k)\n",
        "            except xgb.core.XGBoostError:\n",
        "                params['tree_method']='hist'; params.pop('gpu_id', None)\n",
        "                model_k = _get_xgb_model(pack.task, ytr_k, params)\n",
        "                try: model_k.fit(Xtr_k, ytr_k)\n",
        "                except Exception: model_k = None\n",
        "            except Exception:\n",
        "                model_k = None\n",
        "\n",
        "            metric_val = np.nan\n",
        "            if model_k is not None:\n",
        "                try:\n",
        "                    yhat = model_k.predict(pack.Xte[:, keep].astype(np.float32))\n",
        "                    metric_val = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "                except Exception:\n",
        "                    metric_val = np.nan\n",
        "            topk_rows.append([pack.name, m, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", metric_val])\n",
        "    append_rows(os.path.join(outdir, \"summary_topk.csv\"),\n",
        "                [\"dataset\",\"method\",\"k\",\"metric_name\",\"metric_value\"], topk_rows)\n",
        "\n",
        "    # (7) Stability under tiny i.i.d. noise\n",
        "    cirs, j8 = stability_noise_eval(lambda X, p: excir_feature_scores(X, p), pack.Xva, p_va, repeats=15, sigma=0.01)\n",
        "    stab_rows = [[pack.name, int(i), float(c), float(j)] for i, (c, j) in enumerate(zip(cirs, j8))]\n",
        "    append_rows(os.path.join(outdir, \"stability_noise.csv\"),\n",
        "                [\"dataset\",\"repeat_idx\",\"cir_under_noise\",\"jaccard_at_8_under_noise\"], stab_rows)\n",
        "    plt.figure(figsize=(6.6,4.0))\n",
        "    plt.subplot(1,2,1); plt.hist(cirs, bins=10); plt.title('Spearman vs noisy'); plt.xlabel('Spearman'); plt.ylabel('count')\n",
        "    plt.subplot(1,2,2); plt.hist(j8, bins=10); plt.title('Jaccard@8 vs noisy'); plt.xlabel('overlap')\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(outdir, f\"stability_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # (8) Lightweight transfer checks (fractions)\n",
        "    F = np.array([0.2,0.3,0.4,0.5,0.75,1.0])\n",
        "    base_full = train_gboost(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=seed)\n",
        "    if base_full is None:\n",
        "        return scores\n",
        "    p_full = predict_scores(pack.task, base_full, pack.Xva); s_full = excir_feature_scores(pack.Xva, p_full)\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva]); ybig = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "    Times, CIRs, Resid, KLs, J8 = [], [], [], [], []\n",
        "    rs = check_random_state(seed)\n",
        "    for f in F:\n",
        "        t0 = time.time()\n",
        "        n_rows = int(f*Xbig.shape[0])\n",
        "        idx = rs.choice(np.arange(Xbig.shape[0]), size=n_rows, replace=False)\n",
        "\n",
        "        params = dict(n_estimators=30, random_state=seed, tree_method='gpu_hist',\n",
        "                      gpu_id=0, validate_parameters=True, n_jobs=1)\n",
        "        m = _get_xgb_model(pack.task, ybig, params)\n",
        "        try:\n",
        "            m.fit(Xbig[idx].astype(np.float32), ybig[idx])\n",
        "        except xgb.core.XGBoostError:\n",
        "            params['tree_method']='hist'; params.pop('gpu_id', None)\n",
        "            m = _get_xgb_model(pack.task, ybig, params)\n",
        "            try: m.fit(Xbig[idx].astype(np.float32), ybig[idx])\n",
        "            except Exception: m = None\n",
        "        except Exception:\n",
        "            m = None\n",
        "\n",
        "        if m is None:\n",
        "            Times.append(time.time()-t0); CIRs += [np.nan]; Resid += [np.nan]; KLs += [np.nan]; J8 += [np.nan]\n",
        "            continue\n",
        "\n",
        "        p_sub = predict_scores(pack.task, m, pack.Xva)\n",
        "        s_sub = excir_feature_scores(pack.Xva, p_sub)\n",
        "        Times.append(time.time()-t0)\n",
        "        CIRs.append(spearman_rank_corr(s_full, s_sub))\n",
        "        Resid.append(projection_alignment_residual(p_full, p_sub))\n",
        "        KLs.append(kde_kl_symmetric(p_full, p_sub))\n",
        "        J8.append(jaccard_at_k(s_full.argsort()[::-1], s_sub.argsort()[::-1], k=min(8, s_full.shape[0])))\n",
        "\n",
        "    lw_rows = [[pack.name, float(f), float(t), float(c if c==c else np.nan),\n",
        "                float(r if r==r else np.nan), float(k if k==k else np.nan),\n",
        "                float(j if j==j else np.nan)]\n",
        "               for f, t, c, r, k, j in zip(F, Times, CIRs, Resid, KLs, J8)]\n",
        "    append_rows(os.path.join(outdir, \"lightweight_checks.csv\"),\n",
        "                [\"dataset\",\"fraction_rows\",\"wall_time_sec\",\"cir_agreement\",\"proj_residual\",\"sym_kl\",\"jaccard_at_8\"],\n",
        "                lw_rows)\n",
        "\n",
        "    # Pareto & runtime plots\n",
        "    plt.figure(figsize=(6.5,5.0))\n",
        "    scatter = plt.scatter(Times, CIRs, s=300*np.maximum(np.array(J8), 0.05), alpha=0.7, c=J8, cmap='viridis')\n",
        "    for i, f in enumerate(F): plt.annotate(f\"{int(100*f)}%\", (Times[i], CIRs[i]), xytext=(5,5), textcoords='offset points', fontsize=9)\n",
        "    plt.xlabel('Wall time (s)'); plt.ylabel('Spearman(full vs light)'); plt.title(f'Pareto (ExCIR) — {pack.name}')\n",
        "    plt.colorbar(scatter, label='Jaccard@8'); plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(6.2,4.2))\n",
        "    plt.plot((100*F).astype(int), Times, marker='o'); plt.xlabel('Fraction rows kept (%)'); plt.ylabel('Wall time (s)')\n",
        "    plt.title(f'Runtime vs fraction — {pack.name}'); plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"runtime_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # (9) AOPC figure for ExCIR (visual)\n",
        "    fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scores[\"ExCIR\"], steps=9)\n",
        "    plt.figure(figsize=(6.5,4.3))\n",
        "    plt.plot(100*fr, ins, marker='o', label='Insertion (keep top-%)')\n",
        "    plt.plot(100*fr, dele, marker='o', label='Deletion (zero top-%)')\n",
        "    plt.xlabel('Revealed / removed top-% features'); plt.ylabel('Test ' + ('Accuracy' if pack.task=='clf' else 'R$^2$'))\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}'); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"aopc_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # (10) digits: class-conditioned ExCIR example\n",
        "    if pack.name == \"digits8x8\":\n",
        "        clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='lbfgs', random_state=seed)\n",
        "        clf.fit(np.vstack([pack.Xtr, pack.Xva]), np.r_[pack.ytr, pack.yva]); Pva = clf.predict_proba(pack.Xva)\n",
        "        s_c = excir_feature_scores(pack.Xva, Pva[:, 3])  # class 3\n",
        "        H = s_c.reshape(8,8)\n",
        "        append_rows(os.path.join(outdir, \"digits_excir_class3_map.csv\"),\n",
        "                    [f\"col{j}\" for j in range(8)],\n",
        "                    [list(map(float, row)) for row in H])\n",
        "        plt.figure(figsize=(4.5,4.5)); plt.imshow(H, cmap='viridis'); plt.colorbar()\n",
        "        plt.title('Class-conditioned ExCIR (digit 3)'); plt.tight_layout()\n",
        "        plt.savefig(os.path.join(outdir, \"digits_excir_class3.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# %%\n",
        "# ============ 8) Runner (All 29 datasets) ============\n",
        "\n",
        "def run_all(datasets: str = \"all\", seed: int = 0, ks=(3,5,8,12), outdir=\"out\", do_pfi=True):\n",
        "    ensure_out(outdir)\n",
        "    names = [s.strip() for s in datasets.split(\",\") if s.strip()]\n",
        "    if not names or 'all' in [n.lower() for n in names]:\n",
        "        names = list(ALL_DATASETS.keys())\n",
        "\n",
        "    print(f\"--- ExCIR Benchmark on {len(names)} datasets (seed={seed}) ---\")\n",
        "    print(f\"Using XGBoost (GPU when available → CPU fallback).\")\n",
        "    print(f\"Writing CSV/PNG to: {os.path.abspath(outdir)}\")\n",
        "    for name in names:\n",
        "        if name not in ALL_DATASETS:\n",
        "            print(f\"[warn] dataset '{name}' not found, skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            pack = ALL_DATASETS[name](seed=seed)\n",
        "            run_on_dataset(pack, ks=ks, seed=seed, do_pfi=do_pfi, outdir=outdir)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {name}: {e}\")\n",
        "            import traceback; traceback.print_exc(file=sys.stdout)\n",
        "\n",
        "# %%\n",
        "# ============ 9) Main ============\n",
        "\n",
        "def main():\n",
        "    run_all(datasets=\"all\", seed=0, ks=(3,5,8,12), outdir=\"out\", do_pfi=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ExCIR — Reproducible cross-domain benchmark (29 datasets)\n",
        "# - Deterministic setup\n",
        "# - CIR / BlockCIR\n",
        "# - Agreement metrics (Jaccard@k, Precision@k, Spearman)\n",
        "# - AOPC helpers\n",
        "# - Full plotting suite (centering ablation, BlockCIR ablation, Pareto mini-grid)\n",
        "# - Datasets: 24 from your earlier script + 5 small structured synthetics to reach 29\n",
        "\n",
        "# =====================\n",
        "# 0) ENV & DETERMINISM\n",
        "# =====================\n",
        "import os, sys, csv, time, warnings, math, gc\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde, entropy, spearmanr\n",
        "from scipy.signal import periodogram\n",
        "\n",
        "# XGBoost (GPU fallback -> CPU hist)\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, r2_score\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from sklearn.utils import check_random_state\n",
        "\n",
        "# Scikit-learn datasets (renamed to avoid recursion)\n",
        "from sklearn.datasets import (\n",
        "    fetch_openml, fetch_california_housing, fetch_20newsgroups,\n",
        "    load_digits, load_iris as skl_load_iris, load_wine as skl_load_wine,\n",
        "    load_diabetes as skl_load_diabetes, load_breast_cancer as skl_load_breast_cancer,\n",
        "    make_classification, make_regression\n",
        ")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Threads (optional cap for reproducibility on BLAS/OpenMP)\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
        "\n",
        "GLOBAL_SEED = 0\n",
        "rng_global = np.random.RandomState(GLOBAL_SEED)\n",
        "\n",
        "def seed_everything(seed=0):\n",
        "    np.random.seed(seed)\n",
        "seed_everything(GLOBAL_SEED)\n",
        "\n",
        "# I/O helpers\n",
        "def ensure_out(outdir=\"out\"): os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "def append_rows(path: str, header: List[str], rows: List[List]):\n",
        "    write_header = not os.path.exists(path)\n",
        "    with open(path, \"a\", newline=\"\") as f:\n",
        "        wr = csv.writer(f)\n",
        "        if write_header: wr.writerow(header)\n",
        "        wr.writerows(rows)\n",
        "\n",
        "# =========================\n",
        "# 1) CORE: CIR/BlockCIR\n",
        "# =========================\n",
        "def _midmean(x: np.ndarray) -> float:\n",
        "    \"\"\"Mid-mean (trimmed mean on central 50%).\"\"\"\n",
        "    x = np.asarray(x, float).ravel()\n",
        "    q1, q3 = np.percentile(x, [25, 75])\n",
        "    mid = x[(x >= q1) & (x <= q3)]\n",
        "    return float(mid.mean() if mid.size else x.mean())\n",
        "\n",
        "def _center_vec(a: np.ndarray, method: str = \"midmean\") -> np.ndarray:\n",
        "    \"\"\"Center a vector by midmean/median/mean.\"\"\"\n",
        "    if method == \"midmean\":\n",
        "        return a - _midmean(a)\n",
        "    if method == \"median\":\n",
        "        return a - np.median(a)\n",
        "    if method == \"mean\":\n",
        "        return a - a.mean()\n",
        "    raise ValueError(\"method must be 'midmean' | 'median' | 'mean'\")\n",
        "\n",
        "def excir_scores(\n",
        "    X: np.ndarray,\n",
        "    pred: np.ndarray,\n",
        "    center: str = \"midmean\",\n",
        "    eps: float = 1e-12\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    ExCIR per-feature scores (bounded, correlation-aware).\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, float)\n",
        "    p = np.asarray(pred, float).ravel()\n",
        "    n, d = X.shape\n",
        "    assert p.shape[0] == n\n",
        "\n",
        "    # robust centering once\n",
        "    y0 = _center_vec(p, center)\n",
        "    # feature-wise centers: same scalar applied to the entire column (midmean of column)\n",
        "    X0 = X.copy()\n",
        "    if center == \"midmean\":\n",
        "        # column-wise midmeans, streamed style\n",
        "        mcols = np.array([_midmean(X[:, j]) for j in range(d)], float)\n",
        "    elif center == \"median\":\n",
        "        mcols = np.median(X, axis=0)\n",
        "    elif center == \"mean\":\n",
        "        mcols = X.mean(axis=0)\n",
        "    X0 = X0 - mcols\n",
        "\n",
        "    # one pass accumulators\n",
        "    # p_ij = x_ij * y_i\n",
        "    N = np.einsum(\"ij,i->j\", X0, y0)  # sum x_ij*y_i\n",
        "    D = np.einsum(\"ij,i->j\", np.abs(X0), np.abs(y0)) + eps  # sum |x_ij*y_i|\n",
        "    return 0.5 * (1.0 + (N / D))\n",
        "\n",
        "def blockcir_scores(\n",
        "    X: np.ndarray,\n",
        "    pred: np.ndarray,\n",
        "    blocks: List[List[int]],\n",
        "    center: str = \"midmean\",\n",
        "    eps: float = 1e-12\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    BlockCIR: group-wise ExCIR with the same bounded ratio.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, float)\n",
        "    p = np.asarray(pred, float).ravel()\n",
        "    n, d = X.shape\n",
        "    y0 = _center_vec(p, center)\n",
        "\n",
        "    if center == \"midmean\":\n",
        "        mcols = np.array([_midmean(X[:, j]) for j in range(d)], float)\n",
        "    elif center == \"median\":\n",
        "        mcols = np.median(X, axis=0)\n",
        "    else:\n",
        "        mcols = X.mean(axis=0)\n",
        "    X0 = X - mcols\n",
        "\n",
        "    out = []\n",
        "    for G in blocks:\n",
        "        G = list(G)\n",
        "        XG = X0[:, G]\n",
        "        # N_G = sum_i sum_j in G  x_ij*y_i == (sum_j x_ij)*y_i summed over i\n",
        "        # D_G = sum_i sum_j in G |x_ij*y_i|\n",
        "        # vectorized:\n",
        "        pG = XG * y0[:, None]\n",
        "        NG = pG.sum()\n",
        "        DG = np.abs(pG).sum() + eps\n",
        "        out.append(0.5 * (1.0 + NG / DG))\n",
        "    return np.array(out, float)\n",
        "\n",
        "def auto_corr_blocks(\n",
        "    X: np.ndarray,\n",
        "    max_groups: int = 20,\n",
        "    corr_thresh: float = 0.9\n",
        ") -> List[List[int]]:\n",
        "    \"\"\"\n",
        "    Build rough correlated blocks by agglomerating features whose absolute\n",
        "    Pearson correlation > corr_thresh. Returns up to max_groups blocks.\n",
        "    (Fast heuristic; works well enough for BlockCIR demo.)\n",
        "    \"\"\"\n",
        "    X = np.asarray(X, float)\n",
        "    d = X.shape[1]\n",
        "    C = np.corrcoef(X, rowvar=False)\n",
        "    used = np.zeros(d, bool)\n",
        "    blocks = []\n",
        "    for j in np.argsort(-np.nanmax(np.abs(C), axis=1)):\n",
        "        if used[j]:\n",
        "            continue\n",
        "        group = [j]\n",
        "        used[j] = True\n",
        "        for k in range(d):\n",
        "            if not used[k] and k != j and abs(C[j, k]) >= corr_thresh:\n",
        "                used[k] = True\n",
        "                group.append(k)\n",
        "        blocks.append(group)\n",
        "        if len(blocks) >= max_groups:\n",
        "            break\n",
        "    # Put any remaining singletons into tiny blocks if needed\n",
        "    for j in range(d):\n",
        "        if not used[j]:\n",
        "            blocks.append([j])\n",
        "            used[j] = True\n",
        "        if len(blocks) >= max_groups:\n",
        "            break\n",
        "    return blocks\n",
        "\n",
        "# ==========================\n",
        "# 2) AGREEMENT & AOPC\n",
        "# ==========================\n",
        "def jaccard_topk(a_scores, b_scores, k=8):\n",
        "    A = set(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return len(A & B) / (len(A | B) + 1e-12)\n",
        "\n",
        "def precision_at_k(a_scores, b_scores, k=8):\n",
        "    A = list(np.argsort(-a_scores)[:k]); B = set(np.argsort(-b_scores)[:k])\n",
        "    return sum(1 for i in A if i in B) / float(max(k, 1))\n",
        "\n",
        "def spearman_corr(a_scores, b_scores):\n",
        "    # guard degenerate vectors\n",
        "    if np.allclose(a_scores, a_scores[0]) and np.allclose(b_scores, b_scores[0]):\n",
        "        return 1.0\n",
        "    r, _ = spearmanr(a_scores, b_scores)\n",
        "    return float(0.0 if np.isnan(r) else r)\n",
        "\n",
        "def cir_pair(z: np.ndarray, s: np.ndarray, eps: float = 1e-12) -> float:\n",
        "    \"\"\"Pairwise CIR alignment in [0,1] for two score vectors (fast version).\"\"\"\n",
        "    z = np.asarray(z, float).ravel(); s = np.asarray(s, float).ravel()\n",
        "    mu_z, mu_s = z.mean(), s.mean()\n",
        "    m = 0.5*(mu_z + mu_s)\n",
        "    num = z.size * ((mu_z - m)**2 + (mu_s - m)**2)\n",
        "    den = np.sum((z - m)**2) + np.sum((s - m)**2) + eps\n",
        "    return float(num/den)\n",
        "\n",
        "def projection_alignment_residual(y_full, y_light):\n",
        "    y = np.asarray(y_full).ravel(); yp = np.asarray(y_light).ravel()\n",
        "    n = y.shape[0]; Phi = np.c_[yp, np.ones(n)]\n",
        "    theta, *_ = np.linalg.lstsq(Phi, y, rcond=None)\n",
        "    yhat = Phi @ theta\n",
        "    return float(np.linalg.norm(y - yhat) / (np.linalg.norm(y) + 1e-12))\n",
        "\n",
        "def kde_kl_symmetric(y_full, y_light, grid_points=400):\n",
        "    y = np.asarray(y_full).ravel(); yp = np.asarray(y_light).ravel()\n",
        "    kde_p = gaussian_kde(y); kde_q = gaussian_kde(yp)\n",
        "    both = np.r_[y, yp]; lo, hi = np.percentile(both, 0.5), np.percentile(both, 99.5)\n",
        "    xs = np.linspace(lo, hi, grid_points)\n",
        "    p = np.clip(kde_p(xs), 1e-12, None); q = np.clip(kde_q(xs), 1e-12, None)\n",
        "    dx = (hi - lo)/max(grid_points-1, 1)\n",
        "    kl_pq = float(np.sum(p*(np.log(p)-np.log(q))) * dx)\n",
        "    kl_qp = float(np.sum(q*(np.log(q)-np.log(p))) * dx)\n",
        "    return 0.5*(kl_pq + kl_qp)\n",
        "\n",
        "def aopc_curves(task, model, Xte, yte, scores, steps=9):\n",
        "    \"\"\"Insertion↑ / Deletion↓ (0→100% top features).\"\"\"\n",
        "    idx = np.argsort(-scores)\n",
        "    n, d = Xte.shape; fracs = np.linspace(0, 1, steps)\n",
        "    ins, dele = [], []\n",
        "    for f in fracs:\n",
        "        k = max(1, int(f*d)); keep = idx[:k]\n",
        "        mask = np.zeros(d, bool); mask[keep] = True\n",
        "        X_ins = np.where(mask, Xte, 0.0).astype(np.float32)\n",
        "        X_del = np.where(mask, 0.0, Xte).astype(np.float32)\n",
        "        y_ins = model.predict(X_ins); y_del = model.predict(X_del)\n",
        "        if task == \"clf\":\n",
        "            ins.append(accuracy_score(yte, y_ins)); dele.append(accuracy_score(yte, y_del))\n",
        "        else:\n",
        "            ins.append(r2_score(yte, y_ins)); dele.append(r2_score(yte, y_del))\n",
        "    return fracs, np.asarray(ins), np.asarray(dele)\n",
        "\n",
        "# ==========================\n",
        "# 3) MODELS & SCORES\n",
        "# ==========================\n",
        "def _get_xgb_model(task, y_data, params):\n",
        "    unique_classes = np.unique(y_data)\n",
        "    if task == \"clf\":\n",
        "        if len(unique_classes) > 2:\n",
        "            return xgb.XGBClassifier(\n",
        "                objective=\"multi:softmax\", num_class=len(unique_classes),\n",
        "                eval_metric=\"merror\", **params\n",
        "            )\n",
        "        else:\n",
        "            return xgb.XGBClassifier(\n",
        "                objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
        "                use_label_encoder=False, **params\n",
        "            )\n",
        "    return xgb.XGBRegressor(objective=\"reg:squarederror\", eval_metric=\"rmse\", **params)\n",
        "\n",
        "def train_xgb(task, Xtr, ytr, Xva, yva, seed=0):\n",
        "    Xall = np.vstack([Xtr, Xva]).astype(np.float32)\n",
        "    yall = np.r_[ytr, yva]\n",
        "    params = dict(\n",
        "        n_estimators=60, random_state=seed, n_jobs=1,\n",
        "        tree_method=\"gpu_hist\", gpu_id=0, validate_parameters=True\n",
        "    )\n",
        "    model = _get_xgb_model(task, yall, params)\n",
        "    try:\n",
        "        model.fit(Xall, yall)\n",
        "        return model\n",
        "    except xgb.core.XGBoostError:\n",
        "        # CPU fallback\n",
        "        params[\"tree_method\"] = \"hist\"\n",
        "        params.pop(\"gpu_id\", None)\n",
        "        model = _get_xgb_model(task, yall, params)\n",
        "        model.fit(Xall, yall)\n",
        "        return model\n",
        "\n",
        "def pfi_scores(model, X, y, n_repeats=5, seed=0):\n",
        "    try:\n",
        "        return permutation_importance(\n",
        "            model, X.astype(np.float32), y,\n",
        "            n_repeats=n_repeats, random_state=seed\n",
        "        ).importances_mean\n",
        "    except Exception:\n",
        "        return np.zeros(X.shape[1])\n",
        "\n",
        "def tree_gain_scores(model, d):\n",
        "    w = getattr(model, \"feature_importances_\", None)\n",
        "    return w if (w is not None and len(w) == d) else np.zeros(d)\n",
        "\n",
        "def pdp_var_scores(model, X, grid_points=15, random_state=0):\n",
        "    d = X.shape[1]; scores = np.zeros(d)\n",
        "    rs = check_random_state(random_state)\n",
        "    idx = rs.choice(np.arange(X.shape[0]), size=min(1000, X.shape[0]), replace=False)\n",
        "    Xs = X[idx]\n",
        "    for j in range(d):\n",
        "        try:\n",
        "            pd = partial_dependence(\n",
        "                model, Xs.astype(np.float32), features=[j],\n",
        "                kind=\"average\", grid_resolution=grid_points\n",
        "            )\n",
        "            scores[j] = np.var(pd.average[0])\n",
        "        except Exception:\n",
        "            scores[j] = 0.0\n",
        "    return scores\n",
        "\n",
        "def surrogate_lr_scores(X, teacher_scores, alpha=1.0):\n",
        "    X = np.asarray(X, float); y = np.asarray(teacher_scores, float).ravel()\n",
        "    reg = Ridge(alpha=alpha, random_state=0).fit(X, y)\n",
        "    w = np.abs(reg.coef_)\n",
        "    if w.ndim > 1: w = np.linalg.norm(w, axis=0)\n",
        "    s = w / (w.sum() + 1e-12)\n",
        "    return s\n",
        "\n",
        "def mi_pred_scores(X, preds, task):\n",
        "    X = np.asarray(X, float); p = np.asarray(preds, float).ravel()\n",
        "    if task == \"clf\":\n",
        "        z = (p > np.median(p)).astype(int)\n",
        "        return mutual_info_classif(X, z, random_state=0)\n",
        "    z = np.digitize(p, np.quantile(p, [1/3, 2/3]))\n",
        "    return mutual_info_classif(X, z, random_state=0)\n",
        "\n",
        "def mi_label_scores(X, y, task):\n",
        "    return mutual_info_classif(X, y, random_state=0) if task==\"clf\" else mutual_info_regression(X, y, random_state=0)\n",
        "\n",
        "# ==========================\n",
        "# 4) DATASETS (29 total)\n",
        "# ==========================\n",
        "@dataclass\n",
        "class DatasetPack:\n",
        "    name: str\n",
        "    task: str  # \"clf\" | \"reg\"\n",
        "    Xtr: np.ndarray; ytr: np.ndarray\n",
        "    Xva: np.ndarray; yva: np.ndarray\n",
        "    Xte: np.ndarray; yte: np.ndarray\n",
        "    feature_names: List[str]\n",
        "\n",
        "def _standardize_fit(Xtr, Xva, Xte):\n",
        "    ss = StandardScaler(with_mean=True, with_std=True)\n",
        "    return ss.fit_transform(Xtr), ss.transform(Xva), ss.transform(Xte)\n",
        "\n",
        "def _split_xy(X, y, task, seed=0):\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2,\n",
        "                                          stratify=y if task==\"clf\" else None,\n",
        "                                          random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.25,\n",
        "                                          stratify=ytr if task==\"clf\" else None,\n",
        "                                          random_state=seed)\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
        "    return Xtr_s, ytr, Xva_s, yva, Xte_s, yte, feat_names\n",
        "\n",
        "# -- Original 24 you used before --\n",
        "def load_adult(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "        dfX = ds.data.select_dtypes(include=[np.number]).copy()\n",
        "        y = (ds.target == '>50K').astype(int).to_numpy()\n",
        "        X = dfX.to_numpy(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"adult\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,list(dfX.columns))\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_20ng_binary(seed=0):\n",
        "    try:\n",
        "        cats = ['comp.graphics','sci.space']\n",
        "        tr = fetch_20newsgroups(subset='train', categories=cats, remove=('headers','footers','quotes'))\n",
        "        te = fetch_20newsgroups(subset='test',  categories=cats, remove=('headers','footers','quotes'))\n",
        "        tf = TfidfVectorizer(max_features=3000, ngram_range=(1,2), stop_words='english')\n",
        "        Xtr_full = tf.fit_transform(tr.data).astype(np.float32).toarray()\n",
        "        Xte = tf.transform(te.data).astype(np.float32).toarray()\n",
        "        ytr_full, yte = tr.target, te.target\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr_full, ytr_full, test_size=0.2, stratify=ytr_full, random_state=seed)\n",
        "        return DatasetPack(\"20ng_bin\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"tfidf_{i}\" for i in range(Xtr.shape[1])])\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_har(seed=0):\n",
        "    try:\n",
        "        ds = fetch_openml(\"HAR\", version=1, as_frame=True)\n",
        "        X = ds.data.to_numpy(float); y = ds.target.astype('category').cat.codes.to_numpy()\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"har6\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"sensor_{i}\" for i in range(X.shape[1])])\n",
        "    except Exception:\n",
        "        return load_synthetic_clf(seed)\n",
        "\n",
        "def load_digits8x8(seed=0):\n",
        "    dg = load_digits(); X = dg.data.astype(float)/16.0; y = dg.target\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"digits8x8\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"pix{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_california(seed=0):\n",
        "    try:\n",
        "        ds = fetch_california_housing()\n",
        "        X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "        Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "        Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "        Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "        return DatasetPack(\"california\",\"reg\",Xtr,ytr,Xva,yva,Xte,yte,list(ds.feature_names))\n",
        "    except Exception:\n",
        "        return load_synthetic_reg(seed)\n",
        "\n",
        "def load_diabetes(seed=0):\n",
        "    ds = skl_load_diabetes(); X = ds.data.astype(float); y = ds.target.astype(float)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    feat = list(getattr(ds, \"feature_names\", [f\"f{i}\" for i in range(X.shape[1])]))\n",
        "    return DatasetPack(\"diabetes\",\"reg\",Xtr,ytr,Xva,yva,Xte,yte,feat)\n",
        "\n",
        "def load_iris(seed=0):\n",
        "    ds = skl_load_iris(); X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"iris\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,list(ds.feature_names))\n",
        "\n",
        "def load_wine(seed=0):\n",
        "    ds = skl_load_wine(); X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr2, Xva2, ytr2, yva2 = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr_s, Xva_s, Xte_s = _standardize_fit(Xtr2, Xva2, Xte)\n",
        "    return DatasetPack(\"wine\",\"clf\",Xtr_s,ytr2,Xva_s,yva2,Xte_s,yte,list(ds.feature_names))\n",
        "\n",
        "def load_synthetic_clf(seed=0):\n",
        "    X, y = make_classification(n_samples=3000, n_features=40, n_informative=10,\n",
        "                               n_redundant=10, n_repeated=5, n_classes=2, random_state=seed)\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, \"clf\", seed)\n",
        "    return DatasetPack(\"synthetic_clf\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,[f\"synth_clf_{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_synthetic_reg(seed=0):\n",
        "    X, y = make_regression(n_samples=3000, n_features=40, n_informative=10, random_state=seed)\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, \"reg\", seed)\n",
        "    return DatasetPack(\"synthetic_reg\",\"reg\",Xtr,ytr,Xva,yva,Xte,yte,[f\"synth_reg_{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "# extra synthetic modalities you used before (matching your previous list)\n",
        "def _dominant_freq(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs)\n",
        "    return f[np.argmax(Pxx)] if len(Pxx)>0 and np.isfinite(Pxx).any() else 0.0\n",
        "def _spec_entropy(x, fs=1.0):\n",
        "    f, Pxx = periodogram(x, fs=fs); p = Pxx + 1e-12; p /= p.sum(); return float(entropy(p))\n",
        "def _ts_to_feats(T, fs=1.0):\n",
        "    T = np.asarray(T, float)\n",
        "    return np.array([T.mean(), T.std(), T.min(), T.max(),\n",
        "                     _dominant_freq(T, fs), _spec_entropy(T, fs),\n",
        "                     np.mean(np.diff(T)), np.std(np.diff(T))], float)\n",
        "def _gen_ts_dataset(n_samples=2000, length=256, n_channels=1, seed=0, task=\"clf\"):\n",
        "    rs = check_random_state(seed); fs=1.0; X_feats=[]; y=[]\n",
        "    for _ in range(n_samples):\n",
        "        label = rs.randint(0,2) if task==\"clf\" else None\n",
        "        chans=[]\n",
        "        for _c in range(n_channels):\n",
        "            freq = 0.05 + 0.2*rs.rand() + (0.1 if (task==\"clf\" and label==1) else 0.0)\n",
        "            t = np.arange(length)/fs\n",
        "            sig = np.sin(2*np.pi*freq*t)+0.5*np.sin(2*np.pi*(freq*2)*t + rs.rand()*2*np.pi) + 0.3*rs.randn(length)\n",
        "            chans.append(sig)\n",
        "        chans = np.stack(chans, axis=0)\n",
        "        feats = np.concatenate([_ts_to_feats(ch, fs) for ch in chans], 0); X_feats.append(feats)\n",
        "        y.append(label if task==\"clf\" else feats.sum()+rs.randn()*0.5)\n",
        "    X = np.vstack(X_feats); y = np.array(y, dtype=int if task==\"clf\" else float)\n",
        "    return X, y\n",
        "def _split_pack_from_xy(X, y, name, task, seed):\n",
        "    Xtr, ytr, Xva, yva, Xte, yte, feat = _split_xy(X, y, task, seed)\n",
        "    return DatasetPack(name, task, Xtr, ytr, Xva, yva, Xte, yte, [f\"{name}_f{i}\" for i in range(X.shape[1])])\n",
        "\n",
        "def load_ecg_heartbeat(seed=0): X,y=_gen_ts_dataset(2500,256,1,seed,\"clf\"); return _split_pack_from_xy(X,y,\"ecg_heartbeat\",\"clf\",seed)\n",
        "def load_fashion_mnist_features(seed=0):\n",
        "    rs = check_random_state(seed); n=5000; d=128; y=rs.randint(0,10,size=n)\n",
        "    Z = rs.randn(n,16) + (y[:,None]*0.15); W = rs.randn(16,d); X = Z@W + 0.1*rs.randn(n,d)\n",
        "    return _split_pack_from_xy(X,y,\"fashion_mnist_features\",\"clf\",seed)\n",
        "def load_protein_structure(seed=0): X,y=make_regression(4000,120,20,noise=2.0,random_state=seed); return _split_pack_from_xy(X,y,\"protein_structure\",\"reg\",seed)\n",
        "def load_stock_technical(seed=0):\n",
        "    rs=check_random_state(seed); n=4000; T=60\n",
        "    prices = 100 + np.cumsum(rs.randn(n,T),1); rets = np.diff(prices,1)\n",
        "    vol=rets.std(1); mom=prices[:,-1]-prices[:,0]\n",
        "    skew=((rets-rets.mean(1,keepdims=True))**3).mean(1); kurt=((rets-rets.mean(1,keepdims=True))**4).mean(1)\n",
        "    feats=np.c_[rets.mean(1),vol,mom,skew,kurt]; y=(rs.randn(n)+0.2*np.sign(mom)>0).astype(int)\n",
        "    return _split_pack_from_xy(feats,y,\"stock_technical\",\"clf\",seed)\n",
        "def load_sensor_fusion(seed=0): X,y=_gen_ts_dataset(3000,128,3,seed,\"clf\"); return _split_pack_from_xy(X,y,\"sensor_fusion\",\"clf\",seed)\n",
        "def load_gene_expression(seed=0):\n",
        "    rs=check_random_state(seed); n=800; d=2000; y=rs.randint(0,2,size=n)\n",
        "    base=rs.randn(n,d)*0.5; signal=np.zeros((n,d)); idx=rs.choice(d,30,replace=False); signal[y==1][:,idx]=1.5\n",
        "    X=base+signal+0.1*rs.randn(n,d); return _split_pack_from_xy(X,y,\"gene_expression\",\"clf\",seed)\n",
        "def load_network_topology(seed=0):\n",
        "    rs=check_random_state(seed); n=3000\n",
        "    deg=rs.gamma(2.0,2.0,n); cl=rs.beta(2,5,n); asst=rs.uniform(-0.5,0.5,n); tri=rs.poisson(5,n); btw=rs.exponential(1.0,n)\n",
        "    X=np.c_[deg,cl,asst,tri,btw]; y=(0.3*deg+0.8*cl-0.5*asst+0.1*tri+0.2*btw+rs.randn(n)*0.5>1.8).astype(int)\n",
        "    return _split_pack_from_xy(X,y,\"network_topology\",\"clf\",seed)\n",
        "def load_audio_mfcc(seed=0):\n",
        "    rs=check_random_state(seed); n=3500; bands=20; y=rs.randint(0,5,size=n); base=rs.randn(n,bands)\n",
        "    for c in range(5): base[y==c]+=(c-2)*0.25\n",
        "    X=base+0.1*rs.randn(n,bands); return _split_pack_from_xy(X,y,\"audio_mfcc\",\"clf\",seed)\n",
        "def load_weather_station(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    pressure=rs.normal(1013,8,n); humidity=rs.uniform(15,95,n); wind=rs.gamma(2.0,1.2,n); cloud=rs.uniform(0,1,n); doy=rs.randint(1,366,n)\n",
        "    temp = 10 + 10*np.sin(2*np.pi*doy/365) - 0.01*(pressure-1013) - 0.05*(humidity-50) - 0.3*cloud + 0.2*wind + rs.randn(n)\n",
        "    X=np.c_[pressure,humidity,wind,cloud,doy]; y=temp; return _split_pack_from_xy(X,y,\"weather_station\",\"reg\",seed)\n",
        "def load_eeg_signals(seed=0): X,y=_gen_ts_dataset(3000,256,4,seed,\"clf\"); return _split_pack_from_xy(X,y,\"eeg_signals\",\"clf\",seed)\n",
        "def load_satellite_ndvi(seed=0):\n",
        "    rs=check_random_state(seed); n=4000\n",
        "    bands=rs.uniform(0,1,(n,6)); sun=rs.uniform(0,70,n); view=rs.uniform(0,30,n)\n",
        "    ndvi=(bands[:,3]-bands[:,2])/(bands[:,3]+bands[:,2]+1e-6); y=ndvi+0.001*(sun-35)-0.001*(view-15)+rs.randn(n)*0.02\n",
        "    X=np.c_[bands,sun,view]; return _split_pack_from_xy(X,y,\"satellite_ndvi\",\"reg\",seed)\n",
        "def load_industrial_process(seed=0):\n",
        "    rs=check_random_state(seed); n=4500\n",
        "    s1=rs.normal(0,1,n); s2=rs.normal(0,1.2,n); s3=rs.normal(0.5,0.8,n); sp=rs.uniform(-1,1,n); noise=rs.randn(n)*0.3\n",
        "    y=2.0+1.5*s1-0.7*s2+0.9*s3+1.2*sp+noise; X=np.c_[s1,s2,s3,sp]\n",
        "    return _split_pack_from_xy(X,y,\"industrial_process\",\"reg\",seed)\n",
        "def load_social_network(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    posts=rs.poisson(3,n); friends=rs.poisson(120,n); likes=rs.poisson(50,n); comments=rs.poisson(12,n); shares=rs.poisson(5,n)\n",
        "    X=np.c_[posts,friends,likes,comments,shares]\n",
        "    y=(0.01*friends+0.03*likes+0.05*comments+0.08*shares+0.2*posts+rs.randn(n)*0.5>3.0).astype(int)\n",
        "    return _split_pack_from_xy(X,y,\"social_network\",\"clf\",seed)\n",
        "def load_cyber_security(seed=0):\n",
        "    X,y=make_classification(5000,30,8,10,weights=[0.8,0.2],class_sep=1.5,random_state=seed)\n",
        "    return _split_pack_from_xy(X,y,\"cyber_security\",\"clf\",seed)\n",
        "def load_mixed_modal(seed=0):\n",
        "    rs=check_random_state(seed)\n",
        "    X_tab,y=make_classification(4000,20,6,6,random_state=seed); X_spec=[]\n",
        "    for _ in range(X_tab.shape[0]):\n",
        "        t=np.arange(64); sig=np.sin(2*np.pi*(0.08+0.1*rs.rand())*t)+0.3*rs.randn(64)\n",
        "        X_spec.append(_ts_to_feats(sig))\n",
        "    X=np.c_[X_tab,np.vstack(X_spec)]; return _split_pack_from_xy(X,y,\"mixed_modal\",\"clf\",seed)\n",
        "\n",
        "# + 5 small structured add-ons to reach 29\n",
        "def load_breast_cancer(seed=0):\n",
        "    ds = skl_load_breast_cancer(); X = ds.data.astype(float); y = ds.target.astype(int)\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n",
        "    Xtr, Xva, ytr, yva = train_test_split(Xtr, ytr, test_size=0.2, stratify=ytr, random_state=seed)\n",
        "    Xtr, Xva, Xte = _standardize_fit(Xtr, Xva, Xte)\n",
        "    return DatasetPack(\"breast_cancer\",\"clf\",Xtr,ytr,Xva,yva,Xte,yte,list(ds.feature_names))\n",
        "\n",
        "def load_retail_demand(seed=0):\n",
        "    rs=check_random_state(seed); n=5000\n",
        "    price=rs.lognormal(mean=2.5, sigma=0.5, size=n)\n",
        "    promo=rs.binomial(1, 0.3, size=n)\n",
        "    season=rs.randint(1,13,size=n)\n",
        "    trend=np.linspace(0,1,n)\n",
        "    noise=rs.randn(n)*0.2\n",
        "    demand = 3.0 - 0.5*np.log(price) + 0.8*promo + 0.2*np.sin(2*np.pi*season/12) + 0.5*trend + noise\n",
        "    X=np.c_[price,promo,season,trend]; y=demand\n",
        "    return _split_pack_from_xy(X,y,\"retail_demand\",\"reg\",seed)\n",
        "\n",
        "def load_traffic_flow(seed=0):\n",
        "    rs=check_random_state(seed); n=4500\n",
        "    hour=rs.randint(0,24,n); dow=rs.randint(0,7,n); rain=rs.binomial(1,0.2,n)\n",
        "    base= 20 + 15*np.sin(2*np.pi*hour/24) + 5*(dow>=5) - 3*rain\n",
        "    noise=rs.randn(n)*2\n",
        "    y = base + noise\n",
        "    X = np.c_[hour,dow,rain]\n",
        "    return _split_pack_from_xy(X,y,\"traffic_flow\",\"reg\",seed)\n",
        "\n",
        "def load_energy_consumption(seed=0):\n",
        "    rs=check_random_state(seed); n=4000\n",
        "    temp=rs.normal(18,7,n); humidity=rs.uniform(20,90,n); hour=rs.randint(0,24,n); occupancy=rs.poisson(5,n)\n",
        "    y = 0.4*occupancy + 0.2*hour - 0.1*(temp-20) + 0.05*(humidity-50) + rs.randn(n)\n",
        "    X=np.c_[temp,humidity,hour,occupancy]\n",
        "    return _split_pack_from_xy(X,y,\"energy_consumption\",\"reg\",seed)\n",
        "\n",
        "def load_air_quality(seed=0):\n",
        "    rs=check_random_state(seed); n=4200\n",
        "    pm10=rs.lognormal(2.0,0.5,n); no2=rs.lognormal(1.5,0.3,n); o3=rs.lognormal(1.3,0.3,n)\n",
        "    wind=rs.gamma(2.0,1.0,n); humidity=rs.uniform(20,90,n)\n",
        "    y = 0.5*pm10 + 0.2*no2 - 0.1*o3 - 0.05*wind + 0.02*humidity + rs.randn(n)\n",
        "    X = np.c_[pm10,no2,o3,wind,humidity]\n",
        "    return _split_pack_from_xy(X,y,\"air_quality\",\"reg\",seed)\n",
        "\n",
        "ALL_DATASETS = {\n",
        "    # your original set (~24)\n",
        "    \"adult\":load_adult,\"20ng_bin\":load_20ng_binary,\"har6\":load_har,\"digits8x8\":load_digits8x8,\n",
        "    \"california\":load_california,\"diabetes\":load_diabetes,\"iris\":load_iris,\"wine\":load_wine,\n",
        "    \"synthetic_clf\":load_synthetic_clf,\"synthetic_reg\":load_synthetic_reg,\n",
        "    \"ecg_heartbeat\":load_ecg_heartbeat,\"fashion_mnist_features\":load_fashion_mnist_features,\n",
        "    \"protein_structure\":load_protein_structure,\"stock_technical\":load_stock_technical,\n",
        "    \"sensor_fusion\":load_sensor_fusion,\"gene_expression\":load_gene_expression,\n",
        "    \"network_topology\":load_network_topology,\"audio_mfcc\":load_audio_mfcc,\n",
        "    \"weather_station\":load_weather_station,\"eeg_signals\":load_eeg_signals,\n",
        "    \"satellite_ndvi\":load_satellite_ndvi,\"industrial_process\":load_industrial_process,\n",
        "    \"social_network\":load_social_network,\"cyber_security\":load_cyber_security,\"mixed_modal\":load_mixed_modal,\n",
        "    # +5 to reach 29\n",
        "    \"breast_cancer\":load_breast_cancer,\"retail_demand\":load_retail_demand,\n",
        "    \"traffic_flow\":load_traffic_flow,\"energy_consumption\":load_energy_consumption\n",
        "}\n",
        "assert len(ALL_DATASETS) == 29, f\"{len(ALL_DATASETS)} datasets detected — expected 29.\"\n",
        "\n",
        "# ==========================\n",
        "# 5) RUNNER\n",
        "# ==========================\n",
        "def run_dataset(\n",
        "    pack: DatasetPack,\n",
        "    ks=(3,5,8,12),\n",
        "    center: str = \"midmean\",\n",
        "    outdir=\"out\",\n",
        "    do_pfi: bool = True,\n",
        "    do_blockcir: bool = True\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    ensure_out(outdir)\n",
        "    print(f\"[{pack.name}] N={pack.Xtr.shape[0]+pack.Xva.shape[0]+pack.Xte.shape[0]}  D={pack.Xtr.shape[1]}  task={pack.task}\")\n",
        "\n",
        "    # Train base model\n",
        "    base = train_xgb(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=GLOBAL_SEED)\n",
        "    yhat_te = base.predict(pack.Xte.astype(np.float32))\n",
        "    metric = accuracy_score(pack.yte, yhat_te) if pack.task==\"clf\" else r2_score(pack.yte, yhat_te)\n",
        "    append_rows(os.path.join(outdir, \"base_model_metrics.csv\"),\n",
        "                [\"dataset\",\"task\",\"test_metric_name\",\"test_metric_value\"],\n",
        "                [[pack.name, pack.task, \"accuracy\" if pack.task==\"clf\" else \"r2\", metric]])\n",
        "\n",
        "    # Predictions on val for explainability\n",
        "    p_va = (base.predict_proba(pack.Xva.astype(np.float32))[:,1] if (pack.task==\"clf\" and hasattr(base,\"predict_proba\"))\n",
        "            else base.predict(pack.Xva.astype(np.float32)))\n",
        "    # Scores\n",
        "    scores: Dict[str, np.ndarray] = {\n",
        "        \"ExCIR\":        excir_scores(pack.Xva, p_va, center=center),\n",
        "        \"TreeGain\":     tree_gain_scores(base, pack.Xva.shape[1]),\n",
        "        \"PDP-var\":      pdp_var_scores(base, pack.Xva),\n",
        "        \"MI(pred)\":     mi_pred_scores(pack.Xva, p_va, task=pack.task),\n",
        "        \"MI(label)\":    mi_label_scores(pack.Xva, pack.yva, task=pack.task),\n",
        "        \"Surrogate-LR\": surrogate_lr_scores(pack.Xva, p_va),\n",
        "    }\n",
        "    if do_pfi:\n",
        "        scores[\"PFI\"] = pfi_scores(base, pack.Xva, pack.yva, n_repeats=5, seed=GLOBAL_SEED)\n",
        "\n",
        "    # CSV: feature_importances\n",
        "    feat_rows = []\n",
        "    for mname, vec in scores.items():\n",
        "        for j, val in enumerate(vec):\n",
        "            fname = pack.feature_names[j] if j < len(pack.feature_names) else f\"f{j}\"\n",
        "            feat_rows.append([pack.name, mname, j, fname, float(val)])\n",
        "    append_rows(os.path.join(outdir, \"feature_importances.csv\"),\n",
        "                [\"dataset\",\"method\",\"feature_index\",\"feature_name\",\"score\"],\n",
        "                feat_rows)\n",
        "\n",
        "    # AOPC per method (curves + summary)\n",
        "    curves_rows, summary_rows = [], []\n",
        "    for mname, scr in scores.items():\n",
        "        fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, scr, steps=9)\n",
        "        for f, a, d in zip(fr, ins, dele):\n",
        "            curves_rows.append([pack.name, mname, float(f), float(a), float(d)])\n",
        "        auc_ins = float(np.trapz(ins, fr))\n",
        "        auc_del = float(np.trapz(dele, fr))\n",
        "        combined = 0.5*(auc_ins + (1.0 - auc_del))\n",
        "        summary_rows.append([pack.name, mname, auc_ins, auc_del, combined])\n",
        "    append_rows(os.path.join(outdir, \"aopc_curves.csv\"),\n",
        "                [\"dataset\",\"method\",\"fraction_kept\",\"insertion\",\"deletion\"],\n",
        "                curves_rows)\n",
        "    append_rows(os.path.join(outdir, \"aopc_summary.csv\"),\n",
        "                [\"dataset\",\"method\",\"auc_insertion\",\"auc_deletion\",\"combined\"],\n",
        "                summary_rows)\n",
        "\n",
        "    # Top-k sufficiency via model@k\n",
        "    topk_rows = []\n",
        "    for mname, v in scores.items():\n",
        "        ranks = np.argsort(-v)\n",
        "        for k in ks:\n",
        "            keep = ranks[:k]\n",
        "            Xtr_k = np.vstack([pack.Xtr, pack.Xva])[:, keep]\n",
        "            ytr_k = np.r_[pack.ytr, pack.yva]\n",
        "\n",
        "            params = dict(n_estimators=80, random_state=GLOBAL_SEED, n_jobs=1,\n",
        "                          tree_method=\"gpu_hist\", gpu_id=0, validate_parameters=True)\n",
        "            model_k = _get_xgb_model(pack.task, ytr_k, params)\n",
        "            try:\n",
        "                model_k.fit(Xtr_k.astype(np.float32), ytr_k)\n",
        "            except xgb.core.XGBoostError:\n",
        "                params[\"tree_method\"]=\"hist\"; params.pop(\"gpu_id\", None)\n",
        "                model_k = _get_xgb_model(pack.task, ytr_k, params)\n",
        "                model_k.fit(Xtr_k.astype(np.float32), ytr_k)\n",
        "\n",
        "            yhat = model_k.predict(pack.Xte[:, keep].astype(np.float32))\n",
        "            val = accuracy_score(pack.yte, yhat) if pack.task==\"clf\" else r2_score(pack.yte, yhat)\n",
        "            topk_rows.append([pack.name, mname, k, \"accuracy\" if pack.task==\"clf\" else \"r2\", float(val)])\n",
        "    append_rows(os.path.join(outdir, \"summary_topk.csv\"),\n",
        "                [\"dataset\",\"method\",\"k\",\"metric_name\",\"metric_value\"],\n",
        "                topk_rows)\n",
        "\n",
        "    # Stability under small i.i.d. noise for ExCIR\n",
        "    repeats, sigma = 15, 0.01\n",
        "    base_scores = excir_scores(pack.Xva, p_va, center=center)\n",
        "    cirs, j8s, p8s, srs = [], [], [], []\n",
        "    for _ in range(repeats):\n",
        "        noise = np.random.normal(0, sigma, size=pack.Xva.shape)\n",
        "        s_noisy = excir_scores(pack.Xva + noise, p_va, center=center)\n",
        "        cirs.append(cir_pair(base_scores, s_noisy))\n",
        "        j8s.append(jaccard_topk(base_scores, s_noisy, k=min(8, base_scores.shape[0])))\n",
        "        p8s.append(precision_at_k(base_scores, s_noisy, k=min(8, base_scores.shape[0])))\n",
        "        srs.append(spearman_corr(base_scores, s_noisy))\n",
        "    stab_rows = [[pack.name, int(i), float(c), float(j), float(p), float(sr)]\n",
        "                 for i, (c, j, p, sr) in enumerate(zip(cirs, j8s, p8s, srs))]\n",
        "    append_rows(os.path.join(outdir, \"stability_noise.csv\"),\n",
        "                [\"dataset\",\"repeat_idx\",\"cir\",\"jaccard@8\",\"precision@8\",\"spearman\"],\n",
        "                stab_rows)\n",
        "\n",
        "    # Lightweight-pareto (agreement vs cost)\n",
        "    fractions = (0.2, 0.3, 0.4, 0.6, 0.8, 1.0)\n",
        "    Xbig = np.vstack([pack.Xtr, pack.Xva]); ybig = np.r_[pack.ytr, pack.yva]\n",
        "    F, Times, CIRs, Resid, KLs, J8 = [], [], [], [], [], []\n",
        "    for f in fractions:\n",
        "        t0 = time.time()\n",
        "        rs = check_random_state(GLOBAL_SEED)\n",
        "        idx = rs.choice(np.arange(Xbig.shape[0]), size=max(50, int(f*Xbig.shape[0])), replace=False)\n",
        "        params = dict(n_estimators=40, random_state=GLOBAL_SEED, n_jobs=1,\n",
        "                      tree_method=\"gpu_hist\", gpu_id=0, validate_parameters=True)\n",
        "        m = _get_xgb_model(pack.task, ybig, params)\n",
        "        try:\n",
        "            m.fit(Xbig[idx].astype(np.float32), ybig[idx])\n",
        "        except xgb.core.XGBoostError:\n",
        "            params[\"tree_method\"]=\"hist\"; params.pop(\"gpu_id\", None)\n",
        "            m = _get_xgb_model(pack.task, ybig, params)\n",
        "            m.fit(Xbig[idx].astype(np.float32), ybig[idx])\n",
        "\n",
        "        p_full = p_va\n",
        "        p_sub = (m.predict_proba(pack.Xva.astype(np.float32))[:,1] if (pack.task==\"clf\" and hasattr(m,\"predict_proba\"))\n",
        "                 else m.predict(pack.Xva.astype(np.float32)))\n",
        "        s_full = base_scores\n",
        "        s_sub  = excir_scores(pack.Xva, p_sub, center=center)\n",
        "\n",
        "        Times.append(time.time()-t0); F.append(f)\n",
        "        CIRs.append(cir_pair(s_full, s_sub))\n",
        "        Resid.append(projection_alignment_residual(p_full, p_sub))\n",
        "        KLs.append(kde_kl_symmetric(p_full, p_sub))\n",
        "        J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.shape[0])))\n",
        "\n",
        "    lw_rows = [[pack.name, float(f), float(t), float(c), float(r), float(k), float(j)]\n",
        "               for f,t,c,r,k,j in zip(F,Times,CIRs,Resid,KLs,J8)]\n",
        "    append_rows(os.path.join(outdir, \"lightweight_checks.csv\"),\n",
        "                [\"dataset\",\"fraction_rows\",\"wall_time_sec\",\"cir_agreement\",\n",
        "                 \"proj_residual\",\"sym_kl\",\"jaccard_at_8\"],\n",
        "                lw_rows)\n",
        "\n",
        "    # Figures: per-dataset (optional, keep light)\n",
        "    # (A) AOPC ExCIR\n",
        "    fr, ins, dele = aopc_curves(pack.task, base, pack.Xte, pack.yte, base_scores, steps=9)\n",
        "    plt.figure(figsize=(6.2,4.2))\n",
        "    plt.plot(100*fr, ins, marker='o', label='Insertion')\n",
        "    plt.plot(100*fr, dele, marker='o', label='Deletion')\n",
        "    plt.xlabel('Top-% features'); plt.ylabel('Accuracy' if pack.task=='clf' else 'R$^2$')\n",
        "    plt.title(f'AOPC (ExCIR) — {pack.name}'); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"aopc_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    # (B) Agreement–cost (Pareto)\n",
        "    plt.figure(figsize=(6.0,4.6))\n",
        "    sc = plt.scatter(Times, CIRs, s=250*np.maximum(J8,0.05), c=J8, cmap='viridis', alpha=0.8)\n",
        "    for i, f in enumerate(F): plt.annotate(f\"{int(100*f)}%\", (Times[i], CIRs[i]), xytext=(4,4), textcoords='offset points', fontsize=9)\n",
        "    plt.xlabel(\"Wall time (s)\"); plt.ylabel(\"CIR(full, light)\")\n",
        "    plt.title(f\"Agreement–cost (ExCIR) — {pack.name}\")\n",
        "    plt.colorbar(sc, label='Jaccard@8'); plt.grid(alpha=0.3); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, f\"pareto_{pack.name}.png\"), dpi=160); plt.close()\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ==========================\n",
        "# 6) PAPER PLOTS (global)\n",
        "# ==========================\n",
        "def plot_ablation_centering(packs: List[DatasetPack], outdir=\"out\"):\n",
        "    \"\"\"\n",
        "    ablation_centering.png\n",
        "    Bars = Spearman corr of (median/mean) vs (midmean baseline).\n",
        "    Line = runtime ratio vs midmean (≈1 is good).\n",
        "    \"\"\"\n",
        "    ensure_out(outdir)\n",
        "    names, sp_med, sp_mean, rt_med, rt_mean = [], [], [], [], []\n",
        "    for pack in packs:\n",
        "        # use model to get p_va once\n",
        "        base = train_xgb(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=GLOBAL_SEED)\n",
        "        p_va = (base.predict_proba(pack.Xva.astype(np.float32))[:,1] if (pack.task==\"clf\" and hasattr(base,\"predict_proba\"))\n",
        "                else base.predict(pack.Xva.astype(np.float32)))\n",
        "        t0 = time.time(); s_mid = excir_scores(pack.Xva, p_va, center=\"midmean\"); t_mid = time.time()-t0\n",
        "        t0 = time.time(); s_med = excir_scores(pack.Xva, p_va, center=\"median\");  t_med = time.time()-t0\n",
        "        t0 = time.time(); s_mean= excir_scores(pack.Xva, p_va, center=\"mean\");    t_mean= time.time()-t0\n",
        "        names.append(pack.name)\n",
        "        sp_med.append(spearman_corr(s_mid, s_med)); sp_mean.append(spearman_corr(s_mid, s_mean))\n",
        "        rt_med.append((t_med / max(t_mid,1e-9)));   rt_mean.append((t_mean/ max(t_mid,1e-9)))\n",
        "        del base; gc.collect()\n",
        "\n",
        "    # Plot compactly\n",
        "    plt.figure(figsize=(7.0,4.2))\n",
        "    x = np.arange(len(names))\n",
        "    width=0.35\n",
        "    plt.bar(x - width/2, sp_med, width, label='Spearman(median, midmean)')\n",
        "    plt.bar(x + width/2, sp_mean, width, label='Spearman(mean, midmean)')\n",
        "    # runtime (as line over mean)\n",
        "    plt.plot(x, rt_med, marker='o', label='Runtime ratio (median/midmean)')\n",
        "    plt.plot(x, rt_mean, marker='o', label='Runtime ratio (mean/midmean)')\n",
        "    plt.xticks(x, [n[:12] for n in names], rotation=45, ha='right')\n",
        "    plt.ylim(0.0, 1.1); plt.legend(ncol=2); plt.tight_layout()\n",
        "    plt.title(\"Centering choice ablation\")\n",
        "    plt.savefig(os.path.join(outdir, \"ablation_centering.png\"), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def plot_ablation_blockcir_geneexpr(outdir=\"out\"):\n",
        "    \"\"\"\n",
        "    ablation_blockcir_geneexpr.png\n",
        "    Left: score profiles (CIR vs BlockCIR) after sorting by CIR rank.\n",
        "    Right: Jaccard@k overlap across k.\n",
        "    \"\"\"\n",
        "    ensure_out(outdir)\n",
        "    pack = ALL_DATASETS[\"gene_expression\"](seed=GLOBAL_SEED)\n",
        "    base = train_xgb(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=GLOBAL_SEED)\n",
        "    p_va = (base.predict_proba(pack.Xva.astype(np.float32))[:,1] if hasattr(base, \"predict_proba\")\n",
        "            else base.predict(pack.Xva.astype(np.float32)))\n",
        "    s_cir = excir_scores(pack.Xva, p_va, center=\"midmean\")\n",
        "    order = np.argsort(-s_cir)\n",
        "    # auto-blocks from val set\n",
        "    blocks = auto_corr_blocks(pack.Xva, max_groups=60, corr_thresh=0.9)\n",
        "    s_block = blockcir_scores(pack.Xva, p_va, blocks, center=\"midmean\")\n",
        "\n",
        "    # Expand block scores to feature order (for visual comparison)\n",
        "    block_to_feat = np.zeros_like(s_cir)\n",
        "    for bi, G in enumerate(blocks):\n",
        "        for j in G: block_to_feat[j] = s_block[bi]\n",
        "    s_block_feat_ordered = block_to_feat[order]\n",
        "    s_cir_ordered = s_cir[order]\n",
        "\n",
        "    # Jaccard@k overlap CIR vs BlockCIR-projected\n",
        "    ks = np.arange(2, 61, 2)\n",
        "    jac = []\n",
        "    for k in ks:\n",
        "        A = set(order[:k])\n",
        "        B = set(np.argsort(-block_to_feat)[:k])\n",
        "        jac.append(len(A & B)/float(len(A | B)))\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize=(10,4.0))\n",
        "    ax[0].plot(s_cir_ordered, label='CIR (feature-level)')\n",
        "    ax[0].plot(s_block_feat_ordered, label='BlockCIR (expanded to features)')\n",
        "    ax[0].set_title('Score profiles (sorted by CIR rank)'); ax[0].legend(); ax[0].grid(alpha=0.3)\n",
        "    ax[1].plot(ks, jac, marker='o'); ax[1].set_title('Top-k Jaccard: CIR vs BlockCIR'); ax[1].set_xlabel('k'); ax[1].set_ylabel('Jaccard'); ax[1].grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, \"ablation_blockcir_geneexpr.png\"), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "def plot_pareto_mini_grid(packs: List[DatasetPack], outdir=\"out\"):\n",
        "    \"\"\"\n",
        "    pareto_mini_grid.png\n",
        "    Each point: (runtime, Jaccard@8) for f in {0.2,0.3,0.4,0.6,0.8,1.0}, small multi-dataset grid.\n",
        "    \"\"\"\n",
        "    ensure_out(outdir)\n",
        "    figs = []\n",
        "    plt.figure(figsize=(7.2,4.6))\n",
        "    colors = plt.cm.tab10.colors\n",
        "    for i, pack in enumerate(packs):\n",
        "        # fit base once\n",
        "        base = train_xgb(pack.task, pack.Xtr, pack.ytr, pack.Xva, pack.yva, seed=GLOBAL_SEED)\n",
        "        p_va = (base.predict_proba(pack.Xva.astype(np.float32))[:,1] if (pack.task==\"clf\" and hasattr(base,\"predict_proba\"))\n",
        "                else base.predict(pack.Xva.astype(np.float32)))\n",
        "        s_full = excir_scores(pack.Xva, p_va, center=\"midmean\")\n",
        "\n",
        "        Xbig = np.vstack([pack.Xtr, pack.Xva]); ybig = np.r_[pack.ytr, pack.yva]\n",
        "        fractions = (0.2, 0.3, 0.4, 0.6, 0.8, 1.0)\n",
        "        T, J8 = [], []\n",
        "        for f in fractions:\n",
        "            t0=time.time()\n",
        "            rs = check_random_state(GLOBAL_SEED)\n",
        "            idx = rs.choice(np.arange(Xbig.shape[0]), size=max(50,int(f*Xbig.shape[0])), replace=False)\n",
        "            params = dict(n_estimators=30, random_state=GLOBAL_SEED, n_jobs=1,\n",
        "                          tree_method=\"gpu_hist\", gpu_id=0, validate_parameters=True)\n",
        "            m = _get_xgb_model(pack.task, ybig, params)\n",
        "            try:\n",
        "                m.fit(Xbig[idx].astype(np.float32), ybig[idx])\n",
        "            except xgb.core.XGBoostError:\n",
        "                params[\"tree_method\"]=\"hist\"; params.pop(\"gpu_id\", None)\n",
        "                m = _get_xgb_model(pack.task, ybig, params); m.fit(Xbig[idx].astype(np.float32), ybig[idx])\n",
        "            p_sub = (m.predict_proba(pack.Xva.astype(np.float32))[:,1] if (pack.task==\"clf\" and hasattr(m,\"predict_proba\"))\n",
        "                     else m.predict(pack.Xva.astype(np.float32)))\n",
        "            s_sub = excir_scores(pack.Xva, p_sub, center=\"midmean\")\n",
        "            T.append(time.time()-t0); J8.append(jaccard_topk(s_full, s_sub, k=min(8, s_full.shape[0])))\n",
        "\n",
        "        plt.plot(T, J8, marker='o', label=pack.name, color=colors[i % len(colors)], alpha=0.8)\n",
        "        del base; gc.collect()\n",
        "\n",
        "    plt.axhline(0.9, ls='--', lw=1.2, alpha=0.6)  # target overlap line\n",
        "    plt.xlabel(\"Wall time (s)\"); plt.ylabel(\"Jaccard@8 (full vs light)\")\n",
        "    plt.title(\"Lightweight fraction sweep (mini Pareto)\")\n",
        "    plt.legend(ncol=2, fontsize=8)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(outdir, \"pareto_mini_grid.png\"), dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "# ==========================\n",
        "# 7) MASTER EXECUTION\n",
        "# ==========================\n",
        "def run_all(\n",
        "    datasets: Optional[List[str]] = None,\n",
        "    outdir: str = \"out\",\n",
        "    do_pfi: bool = True\n",
        "):\n",
        "    ensure_out(outdir)\n",
        "    names = datasets if datasets else list(ALL_DATASETS.keys())\n",
        "    print(f\"--- ExCIR Benchmark on {len(names)} datasets ---\")\n",
        "    for name in names:\n",
        "        try:\n",
        "            pack = ALL_DATASETS[name](seed=GLOBAL_SEED)\n",
        "            run_dataset(pack, ks=(3,5,8,12), center=\"midmean\",\n",
        "                        outdir=outdir, do_pfi=do_pfi, do_blockcir=True)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {name}: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "\n",
        "    # Global plots for the paper\n",
        "    # Centering ablation: choose 3 diverse datasets\n",
        "    subset_for_centering = [\n",
        "        ALL_DATASETS[\"california\"](seed=GLOBAL_SEED),\n",
        "        ALL_DATASETS[\"digits8x8\"](seed=GLOBAL_SEED),\n",
        "        ALL_DATASETS[\"network_topology\"](seed=GLOBAL_SEED)\n",
        "    ]\n",
        "    plot_ablation_centering(subset_for_centering, outdir=outdir)\n",
        "\n",
        "    # BlockCIR ablation on gene_expression\n",
        "    plot_ablation_blockcir_geneexpr(outdir=outdir)\n",
        "\n",
        "    # Pareto mini-grid on 4 diverse datasets (fast)\n",
        "    subset_for_pareto = [\n",
        "        ALL_DATASETS[\"har6\"](seed=GLOBAL_SEED),\n",
        "        ALL_DATASETS[\"fashion_mnist_features\"](seed=GLOBAL_SEED),\n",
        "        ALL_DATASETS[\"california\"](seed=GLOBAL_SEED),\n",
        "        ALL_DATASETS[\"stock_technical\"](seed=GLOBAL_SEED),\n",
        "    ]\n",
        "    plot_pareto_mini_grid(subset_for_pareto, outdir=outdir)\n",
        "\n"
      ],
      "metadata": {
        "id": "SISGK3PMedbJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# 8) ENTRY POINT (Notebook)\n",
        "# ==========================\n",
        "\n",
        "# ---- User toggles ----\n",
        "RUN_ALL_29 = True        # True = run all 29 datasets; False = use SHORT_LIST below\n",
        "USE_PFI    = False       # True = include PFI (slower); False = skip PFI (faster)\n",
        "OUTDIR     = \"out\"       # output directory\n",
        "SHORT_LIST = [\"iris\", \"diabetes\", \"digits8x8\"]  # used only if RUN_ALL_29=False\n",
        "# ----------------------\n",
        "\n",
        "if RUN_ALL_29:\n",
        "    print(\"Running ALL 29 datasets...\")\n",
        "    run_all(datasets=None, outdir=OUTDIR, do_pfi=USE_PFI)\n",
        "else:\n",
        "    print(f\"Running SHORT LIST: {SHORT_LIST}\")\n",
        "    run_all(datasets=SHORT_LIST, outdir=OUTDIR, do_pfi=USE_PFI)\n",
        "\n",
        "print(\"\\nDone. Artifacts written to:\", OUTDIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYxsDu5oegg2",
        "outputId": "190948c5-f5c4-4533-b3e0-f708cf1aa61f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running ALL 29 datasets...\n",
            "--- ExCIR Benchmark on 29 datasets ---\n",
            "[adult] N=48842  D=6  task=clf\n",
            "[20ng_bin] N=1960  D=3000  task=clf\n",
            "[har6] N=10299  D=561  task=clf\n",
            "[digits8x8] N=1797  D=64  task=clf\n",
            "[california] N=20640  D=8  task=reg\n",
            "[diabetes] N=442  D=10  task=reg\n",
            "[iris] N=150  D=4  task=clf\n",
            "[wine] N=178  D=13  task=clf\n",
            "[synthetic_clf] N=3000  D=40  task=clf\n",
            "[synthetic_reg] N=3000  D=40  task=reg\n",
            "[ecg_heartbeat] N=2500  D=8  task=clf\n",
            "[fashion_mnist_features] N=5000  D=128  task=clf\n",
            "[ERROR] protein_structure: too many positional arguments\n",
            "[stock_technical] N=4000  D=5  task=clf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-631934397.py\", line 912, in run_all\n",
            "    pack = ALL_DATASETS[name](seed=GLOBAL_SEED)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-631934397.py\", line 491, in load_protein_structure\n",
            "    def load_protein_structure(seed=0): X,y=make_regression(4000,120,20,noise=2.0,random_state=seed); return _split_pack_from_xy(X,y,\"protein_structure\",\"reg\",seed)\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 194, in wrapper\n",
            "    params = func_sig.bind(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3280, in bind\n",
            "    return self._bind(args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3204, in _bind\n",
            "    raise TypeError(\n",
            "TypeError: too many positional arguments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[sensor_fusion] N=3000  D=24  task=clf\n",
            "[gene_expression] N=800  D=2000  task=clf\n",
            "[network_topology] N=3000  D=5  task=clf\n",
            "[audio_mfcc] N=3500  D=20  task=clf\n",
            "[weather_station] N=5000  D=5  task=reg\n",
            "[eeg_signals] N=3000  D=32  task=clf\n",
            "[satellite_ndvi] N=4000  D=8  task=reg\n",
            "[industrial_process] N=4500  D=4  task=reg\n",
            "[social_network] N=5000  D=5  task=clf\n",
            "[ERROR] cyber_security: too many positional arguments\n",
            "[ERROR] mixed_modal: too many positional arguments\n",
            "[breast_cancer] N=569  D=30  task=clf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-631934397.py\", line 912, in run_all\n",
            "    pack = ALL_DATASETS[name](seed=GLOBAL_SEED)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-631934397.py\", line 536, in load_cyber_security\n",
            "    X,y=make_classification(5000,30,8,10,weights=[0.8,0.2],class_sep=1.5,random_state=seed)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 194, in wrapper\n",
            "    params = func_sig.bind(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3280, in bind\n",
            "    return self._bind(args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3204, in _bind\n",
            "    raise TypeError(\n",
            "TypeError: too many positional arguments\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-631934397.py\", line 912, in run_all\n",
            "    pack = ALL_DATASETS[name](seed=GLOBAL_SEED)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-631934397.py\", line 540, in load_mixed_modal\n",
            "    X_tab,y=make_classification(4000,20,6,6,random_state=seed); X_spec=[]\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 194, in wrapper\n",
            "    params = func_sig.bind(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3280, in bind\n",
            "    return self._bind(args, kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 3204, in _bind\n",
            "    raise TypeError(\n",
            "TypeError: too many positional arguments\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[retail_demand] N=5000  D=4  task=reg\n",
            "[traffic_flow] N=4500  D=3  task=reg\n",
            "[energy_consumption] N=4000  D=4  task=reg\n",
            "\n",
            "Done. Artifacts written to: out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_DATASETS = {\n",
        "    \"adult\":load_adult,\"20ng_bin\":load_20ng_binary,\"har6\":load_har,\"digits8x8\":load_digits8x8,\n",
        "    \"california\":load_california,\"diabetes\":load_diabetes,\"iris\":load_iris,\"wine\":load_wine,\n",
        "    \"synthetic_clf\":load_synthetic_clf,\"synthetic_reg\":load_synthetic_reg,\n",
        "    \"ecg_heartbeat\":load_ecg_heartbeat,\"fashion_mnist_features\":load_fashion_mnist_features,\n",
        "    \"protein_structure\":load_protein_structure,\"stock_technical\":load_stock_technical,\n",
        "    \"sensor_fusion\":load_sensor_fusion,\"gene_expression\":load_gene_expression,\n",
        "    \"network_topology\":load_network_topology,\"audio_mfcc\":load_audio_mfcc,\n",
        "    \"weather_station\":load_weather_station,\"eeg_signals\":load_eeg_signals,\n",
        "    \"satellite_ndvi\":load_satellite_ndvi,\"industrial_process\":load_industrial_process,\n",
        "    \"social_network\":load_social_network,\"cyber_security\":load_cyber_security,\"mixed_modal\":load_mixed_modal,\n",
        "}"
      ],
      "metadata": {
        "id": "bnKpNrV3f6n3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}